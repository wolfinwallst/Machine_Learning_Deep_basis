{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 9 XOR\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers_MLP(멀티 레이어 퍼셉트론)\n",
    "# layer 한 개(linear2)가 더 추가된 점이 Lab-08_1과 다른 점\n",
    "# xor-nn\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7434073090553284\n",
      "100 0.693165123462677\n",
      "200 0.6931577920913696\n",
      "300 0.6931517124176025\n",
      "400 0.6931463479995728\n",
      "500 0.6931411027908325\n",
      "600 0.6931357383728027\n",
      "700 0.6931295394897461\n",
      "800 0.6931220889091492\n",
      "900 0.6931126117706299\n",
      "1000 0.6930999755859375\n",
      "1100 0.693082332611084\n",
      "1200 0.6930569410324097\n",
      "1300 0.6930190324783325\n",
      "1400 0.6929606199264526\n",
      "1500 0.6928660273551941\n",
      "1600 0.6927032470703125\n",
      "1700 0.6923960447311401\n",
      "1800 0.6917302012443542\n",
      "1900 0.6899654269218445\n",
      "2000 0.6838316321372986\n",
      "2100 0.6561669707298279\n",
      "2200 0.4311031997203827\n",
      "2300 0.1348937451839447\n",
      "2400 0.06630446761846542\n",
      "2500 0.04216821864247322\n",
      "2600 0.030453890562057495\n",
      "2700 0.02366594970226288\n",
      "2800 0.019277770072221756\n",
      "2900 0.016224045306444168\n",
      "3000 0.013983819633722305\n",
      "3100 0.012273941189050674\n",
      "3200 0.01092812605202198\n",
      "3300 0.00984247401356697\n",
      "3400 0.008949029259383678\n",
      "3500 0.008201331831514835\n",
      "3600 0.007566758431494236\n",
      "3700 0.007021680474281311\n",
      "3800 0.006548587698489428\n",
      "3900 0.0061342488043010235\n",
      "4000 0.005768368486315012\n",
      "4100 0.005443035624921322\n",
      "4200 0.005151895806193352\n",
      "4300 0.004889912437647581\n",
      "4400 0.0046528782695531845\n",
      "4500 0.00443745544180274\n",
      "4600 0.004240859765559435\n",
      "4700 0.004060688894242048\n",
      "4800 0.0038950261659920216\n",
      "4900 0.003742204513400793\n",
      "5000 0.003600730560719967\n",
      "5100 0.00346947880461812\n",
      "5200 0.0033473100047558546\n",
      "5300 0.003233389463275671\n",
      "5400 0.003126883879303932\n",
      "5500 0.0030270633287727833\n",
      "5600 0.0029333471320569515\n",
      "5700 0.0028452007099986076\n",
      "5800 0.002762150950729847\n",
      "5900 0.002683782484382391\n",
      "6000 0.0026096515357494354\n",
      "6100 0.002539479173719883\n",
      "6200 0.0024729506112635136\n",
      "6300 0.0024097615387290716\n",
      "6400 0.002349694026634097\n",
      "6500 0.0022925599478185177\n",
      "6600 0.0022380677983164787\n",
      "6700 0.0021860890556126833\n",
      "6800 0.0021364726126194\n",
      "6900 0.0020890082232654095\n",
      "7000 0.0020436232443898916\n",
      "7100 0.002000128384679556\n",
      "7200 0.001958428183570504\n",
      "7300 0.0019184042466804385\n",
      "7400 0.0018799975514411926\n",
      "7500 0.0018430717755109072\n",
      "7600 0.0018075513653457165\n",
      "7700 0.0017733548302203417\n",
      "7800 0.001740422798320651\n",
      "7900 0.0017087101005017757\n",
      "8000 0.0016780965961515903\n",
      "8100 0.001648549921810627\n",
      "8200 0.0016200069803744555\n",
      "8300 0.001592440064996481\n",
      "8400 0.001565798418596387\n",
      "8500 0.001540021039545536\n",
      "8600 0.0015150571707636118\n",
      "8700 0.0014909172896295786\n",
      "8800 0.0014674895210191607\n",
      "8900 0.0014448156580328941\n",
      "9000 0.0014228182844817638\n",
      "9100 0.0014014795888215303\n",
      "9200 0.0013806951465085149\n",
      "9300 0.0013605989515781403\n",
      "9400 0.0013410558458417654\n",
      "9500 0.0013220260152593255\n",
      "9600 0.0013035561423748732\n",
      "9700 0.00128563039470464\n",
      "9800 0.0012681189691647887\n",
      "9900 0.0012511077802628279\n",
      "10000 0.001234517665579915\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[0.00106364]\n",
      " [0.99889404]\n",
      " [0.99889404]\n",
      " [0.00165861]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 계산\n",
    "# True if hypothesis > 0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), \n",
    "          '\\nCorrect: ', predicted.detach().cpu().numpy(), \n",
    "          '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear layer를 총 4개 사용하고 (deeper),\n",
    "hidden layers의 weight을 10씩 사용(wider)하자.\n",
    "\n",
    "이 케이스에서는 cost가 위 케이스에 비해 더 빨리 줄어든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6978083848953247\n",
      "100 0.693110466003418\n",
      "200 0.6931020021438599\n",
      "300 0.6930921673774719\n",
      "400 0.6930808424949646\n",
      "500 0.6930676698684692\n",
      "600 0.6930519342422485\n",
      "700 0.6930330395698547\n",
      "800 0.6930099129676819\n",
      "900 0.6929808855056763\n",
      "1000 0.6929439902305603\n",
      "1100 0.692895770072937\n",
      "1200 0.6928312182426453\n",
      "1300 0.6927415132522583\n",
      "1400 0.6926121115684509\n",
      "1500 0.692415714263916\n",
      "1600 0.6920979022979736\n",
      "1700 0.6915386915206909\n",
      "1800 0.6904336214065552\n",
      "1900 0.687838077545166\n",
      "2000 0.6797102689743042\n",
      "2100 0.640831708908081\n",
      "2200 0.5488755702972412\n",
      "2300 0.5083183646202087\n",
      "2400 0.48752063512802124\n",
      "2500 0.3293827176094055\n",
      "2600 0.020208630710840225\n",
      "2700 0.007720681838691235\n",
      "2800 0.004509110003709793\n",
      "2900 0.003107273718342185\n",
      "3000 0.002338963560760021\n",
      "3100 0.0018599143950268626\n",
      "3200 0.0015352165792137384\n",
      "3300 0.001301888725720346\n",
      "3400 0.0011267683003097773\n",
      "3500 0.000990953529253602\n",
      "3600 0.0008827609708532691\n",
      "3700 0.0007946972036734223\n",
      "3800 0.0007217694655992091\n",
      "3900 0.0006604068912565708\n",
      "4000 0.0006081854226067662\n",
      "4100 0.0005632113898172975\n",
      "4200 0.0005240506725385785\n",
      "4300 0.00048974365927279\n",
      "4400 0.00045944342855364084\n",
      "4500 0.00043248344445601106\n",
      "4600 0.00040835182880982757\n",
      "4700 0.00038660160498693585\n",
      "4800 0.00036699455813504755\n",
      "4900 0.00034915818832814693\n",
      "5000 0.0003328809980303049\n",
      "5100 0.0003179757623001933\n",
      "5200 0.0003043010947294533\n",
      "5300 0.00029172023641876876\n",
      "5400 0.0002800448564812541\n",
      "5500 0.00026924582198262215\n",
      "5600 0.0002592249365989119\n",
      "5700 0.00024986060452647507\n",
      "5800 0.00024112242681439966\n",
      "5900 0.00023297418374568224\n",
      "6000 0.00022530031856149435\n",
      "6100 0.0002180955052608624\n",
      "6200 0.00021131333778612316\n",
      "6300 0.00020493876945693046\n",
      "6400 0.00019889732357114553\n",
      "6500 0.00019317775149829686\n",
      "6600 0.0001877985632745549\n",
      "6700 0.00018265967082697898\n",
      "6800 0.0001778125879354775\n",
      "6900 0.0001731889642542228\n",
      "7000 0.0001687969488557428\n",
      "7100 0.00016460096230730414\n",
      "7200 0.0001606061850907281\n",
      "7300 0.0001568091829540208\n",
      "7400 0.00015317409997805953\n",
      "7500 0.00014966758317314088\n",
      "7600 0.0001463451626477763\n",
      "7700 0.00014311200357042253\n",
      "7800 0.0001400546752847731\n",
      "7900 0.00013711114297620952\n",
      "8000 0.00013429421233013272\n",
      "8100 0.00013159800437279046\n",
      "8200 0.0001289468491449952\n",
      "8300 0.0001264263701159507\n",
      "8400 0.00012400519335642457\n",
      "8500 0.00012165197404101491\n",
      "8600 0.00011939422256546095\n",
      "8700 0.00011725997319445014\n",
      "8800 0.00011512834316818044\n",
      "8900 0.00011308731336612254\n",
      "9000 0.00011113510117866099\n",
      "9100 0.0001092099555535242\n",
      "9200 0.00010740149446064606\n",
      "9300 0.00010558862413745373\n",
      "9400 0.0001038890186464414\n",
      "9500 0.0001022121068672277\n",
      "9600 0.00010058704356197268\n",
      "9700 9.898338612401858e-05\n",
      "9800 9.745910938363522e-05\n",
      "9900 9.60143661359325e-05\n",
      "10000 9.455751569475979e-05\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# xor-nn-wide-deep\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid,\n",
    "                            linear3, sigmoid, linear4, sigmoid).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[7.4389551e-05]\n",
      " [9.9988055e-01]\n",
      " [9.9990404e-01]\n",
      " [8.8267479e-05]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), \n",
    "          '\\nCorrect: ', predicted.detach().cpu().numpy(), \n",
    "          '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "여기서부터는 유튭 강의에만 나오는 내용으로,\n",
    "\n",
    "오차역전파를 step-by-step 구현해보는 코드이다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn Layers: nn.Linear() 2개를 사용\n",
    "w1 = torch.Tensor(2, 2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2, 1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "    # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\n",
    "\n",
    "# sigmoid 함수의 미분\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.0181491374969482\n",
      "100 0.9342283010482788\n",
      "200 0.7055551409721375\n",
      "300 0.694060206413269\n",
      "400 0.6935915946960449\n",
      "500 0.6935281157493591\n",
      "600 0.6934850215911865\n",
      "700 0.6934477686882019\n",
      "800 0.6934151649475098\n",
      "900 0.6933867335319519\n",
      "1000 0.6933616399765015\n",
      "1100 0.6933394074440002\n",
      "1200 0.6933199763298035\n",
      "1300 0.6933026313781738\n",
      "1400 0.6932873725891113\n",
      "1500 0.6932736039161682\n",
      "1600 0.693261444568634\n",
      "1700 0.6932506561279297\n",
      "1800 0.6932408213615417\n",
      "1900 0.6932320594787598\n",
      "2000 0.693224310874939\n",
      "2100 0.6932173371315002\n",
      "2200 0.693211019039154\n",
      "2300 0.6932052969932556\n",
      "2400 0.6932002305984497\n",
      "2500 0.6931954622268677\n",
      "2600 0.6931912899017334\n",
      "2700 0.6931875944137573\n",
      "2800 0.6931840777397156\n",
      "2900 0.6931809186935425\n",
      "3000 0.693178117275238\n",
      "3100 0.6931754946708679\n",
      "3200 0.6931731700897217\n",
      "3300 0.6931710243225098\n",
      "3400 0.693169116973877\n",
      "3500 0.6931672692298889\n",
      "3600 0.69316565990448\n",
      "3700 0.6931642293930054\n",
      "3800 0.693162739276886\n",
      "3900 0.6931615471839905\n",
      "4000 0.6931604146957397\n",
      "4100 0.6931593418121338\n",
      "4200 0.6931583881378174\n",
      "4300 0.6931575536727905\n",
      "4400 0.6931567788124084\n",
      "4500 0.6931559443473816\n",
      "4600 0.6931552886962891\n",
      "4700 0.6931546926498413\n",
      "4800 0.6931542158126831\n",
      "4900 0.6931535601615906\n",
      "5000 0.6931531429290771\n",
      "5100 0.693152666091919\n",
      "5200 0.6931522488594055\n",
      "5300 0.6931518316268921\n",
      "5400 0.6931515336036682\n",
      "5500 0.6931512355804443\n",
      "5600 0.6931509375572205\n",
      "5700 0.6931506395339966\n",
      "5800 0.6931503415107727\n",
      "5900 0.6931501030921936\n",
      "6000 0.6931498646736145\n",
      "6100 0.693149745464325\n",
      "6200 0.6931495070457458\n",
      "6300 0.6931493878364563\n",
      "6400 0.6931491494178772\n",
      "6500 0.6931490302085876\n",
      "6600 0.6931489109992981\n",
      "6700 0.6931487917900085\n",
      "6800 0.6931487321853638\n",
      "6900 0.6931485533714294\n",
      "7000 0.6931484937667847\n",
      "7100 0.6931483745574951\n",
      "7200 0.6931482553482056\n",
      "7300 0.693148136138916\n",
      "7400 0.693148136138916\n",
      "7500 0.6931480765342712\n",
      "7600 0.6931480765342712\n",
      "7700 0.6931478977203369\n",
      "7800 0.6931479573249817\n",
      "7900 0.6931477785110474\n",
      "8000 0.6931478381156921\n",
      "8100 0.6931477785110474\n",
      "8200 0.6931476593017578\n",
      "8300 0.6931476593017578\n",
      "8400 0.693147599697113\n",
      "8500 0.693147599697113\n",
      "8600 0.6931475400924683\n",
      "8700 0.6931475400924683\n",
      "8800 0.6931475400924683\n",
      "8900 0.6931474804878235\n",
      "9000 0.6931474804878235\n",
      "9100 0.6931474208831787\n",
      "9200 0.6931474208831787\n",
      "9300 0.6931474208831787\n",
      "9400 0.6931473612785339\n",
      "9500 0.6931474208831787\n",
      "9600 0.6931473016738892\n",
      "9700 0.6931474208831787\n",
      "9800 0.6931473016738892\n",
      "9900 0.6931473612785339\n",
      "10000 0.6931474208831787\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    # forward\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "\n",
    "    # BCE loss\n",
    "    cost = -torch.mean(Y * torch.log(Y_pred) + \n",
    "                       (1 - Y) * torch.log(1 - Y_pred))\n",
    "    \n",
    "    # back propagation(chain rule)\n",
    "    # Loss derivative\n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "\n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
    "\n",
    "    # Layer 1\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 =d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
    "\n",
    "    # weight 업데이트\n",
    "    w1 = w1 - learning_rate * d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "    w2 = w2 - learning_rate * d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
