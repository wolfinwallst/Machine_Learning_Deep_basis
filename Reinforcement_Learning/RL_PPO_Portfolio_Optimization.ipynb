{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 포트 최적화\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. 데이터 다운로드 및 전처리\n",
    "##############################################\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. 벤치마크: 시가총액 비중 배분 계산\n",
    "##############################################\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "\n",
    "##############################################\n",
    "# 3. 환경(Environment) 정의\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns # 주어진 수익률 데이터\n",
    "        self.n_assets = returns.shape[1]  # 자산 개수\n",
    "        self.current_step = 0\n",
    "        # 관측 공간: 각 자산의 수익률 (실수값 범위)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # 액션 공간: 각 자산의 비중 (0~1 사이)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"환경 초기화 및 첫 번째 상태 반환\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"주어진 액션(포트폴리오 비중)에 대해 보상을 계산하고 다음 상태 반환\"\"\"\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        current_return = self.returns[self.current_step]  # 현재 수익률\n",
    "        reward = np.dot(weights, current_return)  # 포트폴리오 수익률 계산\n",
    "        self.current_step += 1  # 다음 스텝으로 이동\n",
    "        done = self.current_step >= len(self.returns)  # 마지막 스텝 여부 확인\n",
    "        next_state = self.returns[self.current_step] if not done else np.zeros(self.n_assets)  # 종료 시 0 반환\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "returns_np = returns_df.values  # NumPy 배열 변환\n",
    "\n",
    "##############################################\n",
    "# 4. PPO 네트워크 정의\n",
    "##############################################\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"PPO 정책 신경망 (행동 결정)\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # 확률 분포 변환\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        action = self.softmax(logits)  # 확률값 반환\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"PPO 가치 신경망 (상태 가치 예측)\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc3(x)  # 상태 가치 출력\n",
    "        return value\n",
    "\n",
    "# 하이퍼파라미터\n",
    "num_episodes = 50\n",
    "gamma = 0.99\n",
    "epsilon = 0.2         # PPO 클리핑 파라미터 (정책 업데이트 제한, 기본값 0.2)\n",
    "K_epochs = 10         # 하나의 에피소드에서 PPO 업데이트 반복 횟수 (기본값 10)\n",
    "\n",
    "# 네트워크 및 옵티마이저 초기화\n",
    "input_dim = returns_np.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = returns_np.shape[1]\n",
    "\n",
    "actor = Actor(input_dim, hidden_dim, output_dim)\n",
    "critic = Critic(input_dim, hidden_dim)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "##############################################\n",
    "# 5. PPO 학습 함수\n",
    "##############################################\n",
    "def train_ppo(num_episodes, env, actor, critic, actor_optimizer, critic_optimizer, gamma, epsilon, K_epochs):\n",
    "    \"\"\"\n",
    "    PPO 알고리즘을 이용한 학습 함수\n",
    "    - epsilon: PPO 클리핑 파라미터 (정책 업데이트 제한, 기본값 0.2)\n",
    "    - K_epochs: 하나의 에피소드에서 PPO 업데이트 반복 횟수 (기본값 10)\n",
    "\n",
    "    \"\"\"\n",
    "    actor.train() # Actor, Critic 네트워크를 학습 모드로 설정\n",
    "    critic.train()\n",
    "\n",
    "    # 전체 학습 과정 시작 (num_episodes 만큼 반복)\n",
    "    for episode in range(num_episodes):\n",
    "        states, actions, rewards, old_probs = [], [], [], []\n",
    "        state = env.reset() # 환경 초기화 및 첫 번째 상태(state) 가져오기\n",
    "        done = False # 에피소드 종료 여부\n",
    "        \n",
    "        # (3) 하나의 에피소드 동안 반복 (환경이 종료될 때까지)\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            # Actor 네트워크를 통해 행동(action) 예측 (포트폴리오 가중치)\n",
    "            action = actor(state_tensor).squeeze(0)\n",
    "            # 상태, 행동, 행동 확률(old_probs)을 저장 (나중에 업데이트에 활용)\n",
    "            states.append(state)\n",
    "            actions.append(action.detach().numpy())\n",
    "            old_probs.append(action.detach()) \n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        returns = []\n",
    "        R = 0 # 미래 보상의 누적 값\n",
    "        for r in reversed(rewards):  # 보상을 역순으로 반복\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)  # 앞쪽에 삽입하여 정방향 순서로 저장\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        states_tensor = torch.FloatTensor(np.array(states))\n",
    "        old_probs_tensor = torch.stack(old_probs)\n",
    "        \n",
    "        # (6) PPO 업데이트 (K_epochs 만큼 반복)\n",
    "        for _ in range(K_epochs):\n",
    "            # Critic 네트워크를 이용하여 현재 상태의 가치 예측\n",
    "            values = critic(states_tensor).squeeze(1)  # [batch_size] 형태로 변환\n",
    "            advantages = returns - values.detach() # 실제 리턴(returns) - 예측 가치(values)\n",
    "            # Advantage를 정규화하여 안정적인 학습 유도\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # Actor 네트워크에서 새로운 행동 확률 계산\n",
    "            new_probs = actor(states_tensor)\n",
    "            # PPO 손실 함수 계산\n",
    "            # (7) 비율 계산: 새로운 정책의 행동 확률 / 기존 정책의 행동 확률\n",
    "            ratio = (new_probs / old_probs_tensor).prod(dim=1) # .prod(dim=1)는 배열의 요소를 특정 차원(dim=1)을 따라 곱하는 연산으로 각 종목에 대한 개별 행동 확률 비율을 모두 곱해서 최종적인 행동 확률 비율을 만든다는 의미\n",
    "            # (8) PPO 손실의 두 가지 항목 계산: clamp로 clipping 적용\n",
    "            surr1 = ratio * advantages  # 기존 방식의 정책 개선 (Unclipped)\n",
    "            surr2 = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages  # 클리핑 적용된 정책 개선 (Clipped)\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() # (9) PPO Actor 손실: 두 개의 손실 중 작은 값 선택 (Conservative Update)\n",
    "            \n",
    "            critic_loss = F.mse_loss(values, returns)\n",
    "            \n",
    "            actor_optimizer.zero_grad() # (11) Actor 네트워크 업데이트\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            critic_optimizer.zero_grad() # (12) Critic 네트워크 업데이트\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "        \n",
    "        print(f\"PPO Episode {episode} | Reward: {sum(rewards):.4f} | Critic Loss: {critic_loss.item():.4f}\")\n",
    "\n",
    "    return actor\n",
    "\n",
    "actor = train_ppo(num_episodes, HistoricalPortfolioEnv(returns_np), actor, critic, actor_optimizer, critic_optimizer, gamma, epsilon, K_epochs)\n",
    "\n",
    "##############################################\n",
    "# 5-1. 백테스팅: 학습된 Actor 네트워크로 전체 기간에 대해 포트폴리오 수익률 산출\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    actor_model.eval()\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        \n",
    "        next_state, reward, done, _ = env_bt.step(action)\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 6. 백테스팅 및 결과 Plot\n",
    "##############################################\n",
    "ppo_daily_returns = backtest(actor, returns_np)\n",
    "ppo_cumulative = np.cumprod(1 + ppo_daily_returns)\n",
    "\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ppo_daily_returns):]\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "plt.plot(benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "plt.plot(ppo_cumulative, label=\"PPO Agent\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Cumulative returns\")\n",
    "plt.title(\"Backtesting: BM vs PPO\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
