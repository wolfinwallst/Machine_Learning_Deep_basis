{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfinwallst/Machine_Learning_Deep_basis/blob/main/RL_Actor_Critic_CartPole_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymMEBeR0sAzZ"
      },
      "source": [
        "[블로그 글 참조](https://baechu-story.tistory.com/57) 하고, 잘 작동하게 수정했다.\n",
        "\n",
        "추가로 이해 안 되는 부분 설명도 추가함\n",
        "\n",
        "여기서 구현한 방법인 `TD Actor-Critic` 이다:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.distributions import Categorical\n",
        "# Categorical은 discrete 확률분포를 다룰 때 사용"
      ],
      "metadata": {
        "id": "OVhEFaXNUnAE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.0002\n",
        "gamma         = 0.98\n",
        "n_rollout     = 10 # 몇 번의 step 마다 데이터를 업데이트 할지 (업데이트 주기)"
      ],
      "metadata": {
        "id": "XdvhkzF1Unqr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.data = [] # 경험 데이터를 저장할 리스트\n",
        "\n",
        "        self.fc1 = nn.Linear(4,256) # 상태(4차원)를 256차원으로 매핑하는 FC layer\n",
        "        self.fc_pi = nn.Linear(256,2) #  policy 네트웤: 256차원 -> 행동 개수(2)로 매핑하여 행동 확률을 계산\n",
        "        self.fc_v = nn.Linear(256,1) # value 네트웤: 256차원 -> 상태의 가치를 출력 (스칼라)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    # actor 네트웤, 순서: input-fc1-relu-fc_pi-softmax\n",
        "    def pi(self, x, softmax_dim = 0): # 정책 함수: 주어진 상태에서 행동 확률 분포를 반환\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc_pi(x) # (policy) logit 계산\n",
        "        prob = F.softmax(x, dim=softmax_dim)\n",
        "        return prob\n",
        "\n",
        "    # critic 네트웤, 순서: input-fc1-relu-fc_v\n",
        "    def v(self, x): # 가치 함수: 주어진 상태의 가치를 반환\n",
        "        x = F.relu(self.fc1(x))\n",
        "        v = self.fc_v(x)\n",
        "        return v\n",
        "\n",
        "    def put_data(self, transition): # 하나의 transition 데이터를 저장하는 함수\n",
        "        self.data.append(transition)\n",
        "\n",
        "    def make_batch(self): # 저장된 경험 데이터를 배치 텐서로 변환하는 함수\n",
        "        s_lst, a_lst, r_lst, s_next_lst, done_lst = [], [], [], [], []\n",
        "        for transition in self.data:\n",
        "            s, a, r, s_next, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r / 100.0]) # 보상 값을 스케일 조정 (옵션)\n",
        "            s_next_lst.append(s_next)\n",
        "            done_mask = 0.0 if done else 1.0 # 에피소드 종료 여부에 따른 마스킹\n",
        "            done_lst.append([done_mask])\n",
        "\n",
        "        # 리스트를 텐서로 변환\n",
        "        s_batch = torch.tensor(s_lst, dtype=torch.float)\n",
        "        a_batch = torch.tensor(a_lst)\n",
        "        r_batch = torch.tensor(r_lst, dtype=torch.float)\n",
        "        s_next_batch = torch.tensor(s_next_lst, dtype=torch.float)\n",
        "        done_batch = torch.tensor(done_lst, dtype=torch.float)\n",
        "        self.data = []\n",
        "        return s_batch, a_batch, r_batch, s_next_batch, done_batch\n",
        "\n",
        "    def train_net(self):\n",
        "        s, a, r, s_next, done = self.make_batch()\n",
        "        # TD Target 계산 : r + γ * V(s')\n",
        "        td_target = r + gamma * self.v(s_next) * done\n",
        "        # TD 에러 (Advantage) 계산\n",
        "        delta = td_target - self.v(s)\n",
        "\n",
        "        pi = self.pi(s, softmax_dim=1) # 정책 네트워크로부터 각 액션의 확률 pi 계산 (배치 형태)\n",
        "        pi_a = pi.gather(1, a) # 각 상태에서 실제 선택된 행동 a에 해당하는 확률만 추출 (즉 batch에서 뽑은 action의 확률)\n",
        "\n",
        "        # 정책 함수와 가치 함수의 loss를 합산하여 총 손실 계산\n",
        "        # 현재 상태의 가치를 최대화하여 다음 상태의 가치와 차이를 줄이는 방향으로 학습을 진행\n",
        "        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "zYI8uOzXUn1r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pi = self.pi(s, softmax_dim=1)` 에서 softmax_dim=1로 지정하면, 각 행(각 상태)에 대해 확률 분포가 계산된다. 예를 들어, 상태 배치가 (batch_size, num_features) 형태이고, 마지막 출력이 (batch_size, num_actions)라면, 각 row에 대해 softmax를 적용해 각 행동이 선택될 확률을 얻는다.\n",
        "\n",
        "`gather(dim, index)` 함수는 지정된 차원 dim을 따라, index 텐서에 있는 인덱스에 해당하는 값을 모은다.\n",
        "예를 들어, pi가 (batch_size, num_actions) 텐서이고, a가 (batch_size, 1) 텐서라면,\n",
        "pi.gather(1, a)는 각 행(row)마다 a에 기록된 인덱스 위치의 값을 추출하여 (batch_size, 1) 형태의 텐서를 반환한다.\n",
        "\n",
        "`torch.log(pi_a)` 는 수식으로\n",
        "$$ \\log \\pi(a | s) $$\n",
        "이다.\n",
        "\n",
        "위 코드의 `loss`는 policy loss와 value loss의 합으로\n",
        "\n",
        "$$ L = L_{\\text{policy}} + L_{\\text{value}} = \\log \\pi(a | s) \\dot \\delta + \\text{Smooth L1 loss} (V(s), r + \\gamma V(s'))$$\n",
        "\n",
        "를 나타낸다. 이 손실을 최소화하도록 네트워크 파라미터를 업데이트하는 것이 코드의 목적이다.\n",
        "\n",
        "`detach()` 함수는 해당 텐서를 계산 그래프에서 분리시켜, 이 값들이 역전파 시 그래디언트 계산에 영향을 주지 않도록 한다. 이는 정책 손실과 가치 손실을 계산할 때 advantage(𝛿)와 TD target이 상수처럼 취급되게 하여,\n",
        "네트워크 파라미터 업데이트 시 부정확한 그라디언트 전파를 막아준다.\n",
        "\n",
        "delta를 상수 값으로 두기 위해 detach() 사용,\n",
        "마찬가지로 `td_target`을 `detach`하여, 이 값이 역전파 시 가치 네트워크 외의 다른 부분(예: 정책 네트워크)에 영향을 미치지 않도록 한다."
      ],
      "metadata": {
        "id": "57pc55gcVVZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    model = ActorCritic()\n",
        "    print_interval = 20 # 성능 출력 간격\n",
        "    score = 0.0\n",
        "\n",
        "    for n_epi in range(1000): # 1000 번으로 변경\n",
        "        done = False\n",
        "        s = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            # n_rollout 번 만큼 환경과 상호작용 후 학습 진행\n",
        "            for t in range(n_rollout):\n",
        "                prob = model.pi(torch.from_numpy(s).float())\n",
        "                m = Categorical(prob)\n",
        "                a = m.sample().item()\n",
        "                s_next, r, done, info = env.step(a)\n",
        "                model.put_data((s, a, r, s_next, done))\n",
        "\n",
        "                s = s_next\n",
        "                score += r\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            model.train_net() # 저장된 경험 데이터를 이용해 네트웤 업데이트\n",
        "\n",
        "        if n_epi % print_interval==0 and n_epi!=0:\n",
        "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
        "            score = 0.0\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcFYT2KrUtPk",
        "outputId": "221e2708-69bb-4418-9b22-c24e107c5754"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of episode :20, avg score : 24.6\n",
            "# of episode :40, avg score : 22.5\n",
            "# of episode :60, avg score : 23.6\n",
            "# of episode :80, avg score : 20.4\n",
            "# of episode :100, avg score : 23.6\n",
            "# of episode :120, avg score : 25.9\n",
            "# of episode :140, avg score : 24.8\n",
            "# of episode :160, avg score : 23.8\n",
            "# of episode :180, avg score : 28.3\n",
            "# of episode :200, avg score : 29.3\n",
            "# of episode :220, avg score : 31.1\n",
            "# of episode :240, avg score : 24.4\n",
            "# of episode :260, avg score : 33.9\n",
            "# of episode :280, avg score : 37.0\n",
            "# of episode :300, avg score : 41.0\n",
            "# of episode :320, avg score : 37.8\n",
            "# of episode :340, avg score : 43.0\n",
            "# of episode :360, avg score : 44.1\n",
            "# of episode :380, avg score : 52.2\n",
            "# of episode :400, avg score : 64.7\n",
            "# of episode :420, avg score : 54.8\n",
            "# of episode :440, avg score : 68.3\n",
            "# of episode :460, avg score : 52.3\n",
            "# of episode :480, avg score : 60.1\n",
            "# of episode :500, avg score : 84.5\n",
            "# of episode :520, avg score : 97.3\n",
            "# of episode :540, avg score : 116.3\n",
            "# of episode :560, avg score : 129.7\n",
            "# of episode :580, avg score : 157.4\n",
            "# of episode :600, avg score : 145.1\n",
            "# of episode :620, avg score : 111.8\n",
            "# of episode :640, avg score : 153.1\n",
            "# of episode :660, avg score : 222.1\n",
            "# of episode :680, avg score : 182.9\n",
            "# of episode :700, avg score : 177.3\n",
            "# of episode :720, avg score : 239.6\n",
            "# of episode :740, avg score : 226.1\n",
            "# of episode :760, avg score : 180.6\n",
            "# of episode :780, avg score : 134.2\n",
            "# of episode :800, avg score : 167.1\n",
            "# of episode :820, avg score : 197.5\n",
            "# of episode :840, avg score : 251.2\n",
            "# of episode :860, avg score : 219.3\n",
            "# of episode :880, avg score : 218.3\n",
            "# of episode :900, avg score : 279.2\n",
            "# of episode :920, avg score : 215.8\n",
            "# of episode :940, avg score : 292.1\n",
            "# of episode :960, avg score : 226.2\n",
            "# of episode :980, avg score : 236.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 코드를 직접 실행하면 `__name__`의 값이 \"`__main__`\"이 되어 main() 함수가 호출되지만, 다른 모듈에서 import할 때는 자동으로 실행되지 않는다. 이는 파이썬에서 모듈을 깔끔하게 설계하고 재사용할 수 있도록 하는 중요한 패턴이다."
      ],
      "metadata": {
        "id": "z35O0v4vXH5n"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}