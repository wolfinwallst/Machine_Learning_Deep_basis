{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPGëŠ” ì—°ì†ì  í–‰ë™ ê³µê°„ì—ì„œ ê²°ì •ë¡ ì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\n",
    "- ì—¬ê¸°ì„œëŠ” í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì œì•½(ê° ìì‚° ë¹„ì¤‘ì´ 0 ì´ìƒ, ì´í•© 1)ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ Actor ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ì„ softmaxë¥¼ í†µí•´ ì •ê·œí™”í•©ë‹ˆë‹¤.\n",
    "- íƒí—˜ì„ ìœ„í•´ í•™ìŠµ ì‹œì—ëŠ” Actorì˜ ì¶œë ¥ì— Gaussian ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "- ê²½í—˜ ì¬í˜„(Replay Buffer)ê³¼ íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬(target network)ì˜ soft updateë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "DDPG ì—ì´ì „íŠ¸ êµ¬ì„±:\n",
    "\n",
    "- Actor ë„¤íŠ¸ì›Œí¬: ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„ raw logitsë¥¼ ì¶œë ¥í•œ í›„ softmaxë¥¼ ì ìš©í•˜ì—¬, ê° ìì‚°ì˜ íˆ¬ì ë¹„ì¤‘(0 ì´ìƒ, í•©=1)ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- Critic ë„¤íŠ¸ì›Œí¬: ìƒíƒœì™€ í–‰ë™ì„ ì…ë ¥ë°›ì•„ Q-valueë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "- íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ì™€ ê²½í—˜ ì¬í˜„(Replay Buffer)ì„ ì‚¬ìš©í•˜ë©°, í•™ìŠµ ì¤‘ì—ëŠ” íƒí—˜ì„ ìœ„í•´ Gaussian ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ ë° ì—…ë°ì´íŠ¸:\n",
    "\n",
    "- Criticì€ TD ëª©í‘œì™€ í˜„ì¬ Qê°’ ê°„ì˜ MSE ì†ì‹¤ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "- ActorëŠ” Criticì˜ Qê°’ì„ ìµœëŒ€í™”í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "- íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ëŠ” ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸ ë°©ì‹(ğœ Ï„)ìœ¼ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ê°±ì‹ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC ê°„ë‹¨í•œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” via DDPG\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "##############################################\n",
    "# S&P500 ìƒìœ„ 20ì¢…ëª© (ì˜ˆì‹œ í‹°ì»¤)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "# 10ë…„ ì „ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€ì˜ ë°ì´í„°\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# ì¼ë³„ ìˆ˜ìµë¥  ê³„ì‚°\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. ë²¤ì¹˜ë§ˆí¬: ì‹œê°€ì´ì•¡ ë¹„ì¤‘ ë°°ë¶„ ê³„ì‚°\n",
    "##############################################\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "print(\"Benchmark weights (ì‹œê°€ì´ì•¡ ë¹„ì¤‘):\")\n",
    "for t, w in zip(tickers, benchmark_weights):\n",
    "    print(f\"{t}: {w:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# 3. í™˜ê²½(Environment) ì •ì˜\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ê³¼ê±° ì¼ë³„ ìˆ˜ìµë¥  ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” í™˜ê²½.\n",
    "    - ìƒíƒœ(state): í•´ë‹¹ì¼ì˜ ê° ì¢…ëª© ìˆ˜ìµë¥  (vector, shape: [n_assets])\n",
    "    - í–‰ë™(action): ê° ìì‚°ì— í• ë‹¹í•  íˆ¬ì ë¹„ì¤‘ (ì—°ì†ì , ë‹¨, ëª¨ë“  ì›ì†Œ â‰¥0, í•©=1)\n",
    "    - ë³´ìƒ(reward): ì„ íƒí•œ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ë‹¹ì¼ ìˆ˜ìµë¥  (ê° ìì‚° ìˆ˜ìµë¥ ê³¼ ë°°ë¶„ ë¹„ì¤‘ì˜ ë‚´ì )\n",
    "    \"\"\"\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns  # numpy array, shape=(T, n_assets)\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.current_step = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # í–‰ë™ì€ ì—°ì†ì  ë²¡í„° (ì‹¤ì œ actionì€ Actorì˜ softmax ì¶œë ¥ì„ ì‚¬ìš©)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        # í˜¹ì‹œë¼ë„ actionì´ ì •ê·œí™”ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ë³´ì •\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        current_return = self.returns[self.current_step]\n",
    "        reward = np.dot(weights, current_return)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.returns)\n",
    "        if not done:\n",
    "            next_state = self.returns[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.n_assets)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# numpy arrayë¡œ ë³€í™˜\n",
    "returns_np = returns_df.values\n",
    "\n",
    "##############################################\n",
    "# 4. Experience Replay Buffer êµ¬í˜„\n",
    "##############################################\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################################\n",
    "# 5. DDPG ëª¨ë¸ ì •ì˜: Actorì™€ Critic ë„¤íŠ¸ì›Œí¬\n",
    "##############################################\n",
    "# Actor: ìƒíƒœë¥¼ ë°›ì•„ raw logitsë¥¼ ì¶œë ¥í•˜ê³  softmaxë¥¼ í†µí•´ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ (í•©=1) ìƒì„±\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3-softmax\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        action = self.softmax(logits)  # ë³´ì¥: ëª¨ë“  ì›ì†Œê°€ 0 ì´ìƒ, í•©=1\n",
    "        return action\n",
    "\n",
    "# Critic: ìƒíƒœì™€ í–‰ë™ì„ ë°›ì•„ Q-value ì¶œë ¥\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "##############################################\n",
    "# 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”\n",
    "##############################################\n",
    "state_dim = returns_np.shape[1]    # n_assets\n",
    "action_dim = returns_np.shape[1]     # í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ ì°¨ì›\n",
    "hidden_dim = 64\n",
    "\n",
    "# Actorì™€ Critic ë„¤íŠ¸ì›Œí¬\n",
    "actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬\n",
    "target_actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizer\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# ê¸°íƒ€ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "num_episodes = 50\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.005  # íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ soft update ë¹„ìœ¨\n",
    "\n",
    "# íƒí—˜ì„ ìœ„í•œ ë…¸ì´ì¦ˆ (Gaussian noise)\n",
    "def add_noise(action, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "    noisy_action = action + noise\n",
    "    # softmaxë¡œ ì •ê·œí™”í•´ì„œ ì œì•½ ì¡°ê±´(í•©=1, ê° ì›ì†Œ â‰¥ 0) ìœ ì§€\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "##############################################\n",
    "# 7. DDPG í•™ìŠµ ë£¨í”„\n",
    "##############################################\n",
    "env = HistoricalPortfolioEnv(returns_np)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        actor.train()\n",
    "        \n",
    "        # íƒí—˜ì„ ìœ„í•´ ë…¸ì´ì¦ˆ ì¶”ê°€ (í•™ìŠµ ì‹œ)\n",
    "        action = add_noise(action, noise_scale=0.1)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # ë¯¸ë‹ˆë°°ì¹˜ ì—…ë°ì´íŠ¸\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            states_b = torch.FloatTensor(states_b)\n",
    "            actions_b = torch.FloatTensor(actions_b)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1)\n",
    "            next_states_b = torch.FloatTensor(next_states_b)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1)\n",
    "            \n",
    "            # Critic ì—…ë°ì´íŠ¸\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states_b)\n",
    "                target_q = target_critic(next_states_b, next_actions)\n",
    "                y = rewards_b + gamma * target_q * (1 - dones_b)\n",
    "            \n",
    "            current_q = critic(states_b, actions_b)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor ì—…ë°ì´íŠ¸: maximize Q(s, actor(s))\n",
    "            actor_loss = -critic(states_b, actor(states_b)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ soft update\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Episode {episode:3d} | Episode Reward: {episode_reward:.4f}\")\n",
    "\n",
    "print(\"DDPG í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "##############################################\n",
    "# 8. ë°±í…ŒìŠ¤íŒ…: í•™ìŠµëœ Actor ë„¤íŠ¸ì›Œí¬ë¡œ ì „ì²´ ê¸°ê°„ì— ëŒ€í•´ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ì‚°ì¶œ\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    actor_model.eval()\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        # ê²°ì •ë¡ ì  í–‰ë™: ë…¸ì´ì¦ˆ ì—†ì´ actorì˜ ì¶œë ¥ ì‚¬ìš© (ì´ë¯¸ softmax ì ìš©ë¨)\n",
    "        next_state, reward, done, _ = env_bt.step(action)\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "ddpg_daily_returns = backtest(actor, returns_np)\n",
    "ddpg_cumulative = np.cumprod(1 + ddpg_daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 9. ë²¤ì¹˜ë§ˆí¬ ë°±í…ŒìŠ¤íŒ…: BM (ì‹œê°€ì´ì•¡ ë¹„ì¤‘ í¬íŠ¸í´ë¦¬ì˜¤)\n",
    "##############################################\n",
    "# ë²¤ì¹˜ë§ˆí¬ì˜ ì¼ë³„ ìˆ˜ìµë¥ : ê° ì¢…ëª© ìˆ˜ìµë¥ ì— ë²¤ì¹˜ë§ˆí¬ ë¹„ì¤‘ ê³±í•©\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ddpg_daily_returns):]\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "# ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë‚ ì§œ (returns_dfì˜ index ì‚¬ìš©)\n",
    "dates = returns_df.index[-len(ddpg_daily_returns):]\n",
    "\n",
    "##############################################\n",
    "# 10. ê²°ê³¼ Plot: BM vs DDPG\n",
    "##############################################\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "plt.plot(dates, ddpg_cumulative, label=\"DDPG Agent\")\n",
    "plt.xlabel(\"ë‚ ì§œ\")\n",
    "plt.ylabel(\"ëˆ„ì  ìˆ˜ìµë¥ \")\n",
    "plt.title(\"Backtesting: BM vs DDPG\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ì„ì„ ì¶”ê°€í•œ ì½”ë“œ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "##############################################\n",
    "# í¬íŠ¸í´ë¦¬ì˜¤ì— ì‚¬ìš©í•  S&P500 ìƒìœ„ 20ì¢…ëª©ì˜ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "# ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ì„¤ì •: ì˜¤ëŠ˜ ë‚ ì§œì™€ 10ë…„ ì „ë¶€í„°ì˜ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# ê° ë‚ ì§œë³„ë¡œ ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥  ê³„ì‚° (ì „ì¼ ëŒ€ë¹„ ë³€í™”ìœ¨)\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. ë²¤ì¹˜ë§ˆí¬: ì‹œê°€ì´ì•¡ ë¹„ì¤‘ ë°°ë¶„ ê³„ì‚°\n",
    "##############################################\n",
    "# ê° ì¢…ëª©ì˜ ì‹œê°€ì´ì•¡ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë²¤ì¹˜ë§ˆí¬ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ë¹„ì¤‘ì„ ê³„ì‚°\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì„ì˜ì˜ ê°’ í• ë‹¹ (ë°ì´í„° ëˆ„ë½ ë°©ì§€)\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "# ê° ì¢…ëª©ì˜ ì‹œê°€ì´ì•¡ì„ ì „ì²´ ì‹œê°€ì´ì•¡ í•©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë²¤ì¹˜ë§ˆí¬ ë¹„ì¤‘ ê³„ì‚°\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "print(\"Benchmark weights (ì‹œê°€ì´ì•¡ ë¹„ì¤‘):\")\n",
    "for t, w in zip(tickers, benchmark_weights):\n",
    "    print(f\"{t}: {w:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# 3. í™˜ê²½(Environment) ì •ì˜\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ê³¼ê±° ì¼ë³„ ìˆ˜ìµë¥  ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” í™˜ê²½.\n",
    "    - ìƒíƒœ(state): í•´ë‹¹ ì¼ìì˜ ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥  (ë²¡í„°, shape: [n_assets])\n",
    "    - í–‰ë™(action): ê° ìì‚°ì— í• ë‹¹í•  íˆ¬ì ë¹„ì¤‘. ì—°ì†ì ì¸ ê°’ì´ë©° ëª¨ë“  ì›ì†ŒëŠ” 0 ì´ìƒ, í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½ë¨.\n",
    "      (Actor ë„¤íŠ¸ì›¤ì˜ softmax ì¶œë ¥ ê²°ê³¼ë¥¼ ì‚¬ìš©)\n",
    "    - ë³´ìƒ(reward): í•´ë‹¹ ì¼ì í¬íŠ¸í´ë¦¬ì˜¤ì˜ ìˆ˜ìµë¥ , ì¦‰ ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥ ê³¼ íˆ¬ì ë¹„ì¤‘ì˜ ë‚´ì (dot product)\n",
    "    \"\"\"\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns  # numpy array í˜•íƒœë¡œ ì¼ë³„ ìˆ˜ìµë¥  ë°ì´í„°, shape=(T, n_assets)\n",
    "        self.n_assets = returns.shape[1]  # ì¢…ëª©ì˜ ìˆ˜\n",
    "        self.current_step = 0  # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œì  ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "        # ìƒíƒœ ê³µê°„: ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥ , ë¬´í•œëŒ€ ë²”ìœ„ì˜ ì‹¤ìˆ˜ê°’ ê°€ëŠ¥\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # í–‰ë™ ê³µê°„: ê° ì¢…ëª©ì— í• ë‹¹í•  ë¹„ì¤‘, [0,1] ë²”ìœ„ì´ë©° ì‹¤ì œë¡œ softmaxë¥¼ í†µí•´ ì •ê·œí™”ë¨\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"ì—í”¼ì†Œë“œ ì´ˆê¸°í™”: ì‹œì‘ ë‹¨ê³„ë¡œ ëŒì•„ê°€ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ í–‰ë™(íˆ¬ì ë¹„ì¤‘)ì— ë”°ë¼ ë³´ìƒ(ë‹¹ì¼ ìˆ˜ìµë¥ )ì„ ê³„ì‚°í•˜ê³ , ë‹¤ìŒ ìƒíƒœë¡œ ì „í™˜.\n",
    "        - í–‰ë™ì´ ì •ê·œí™”ë˜ì§€ ì•Šì€ ê²½ìš°, softmaxì™€ ìœ ì‚¬í•˜ê²Œ ì •ê·œí™”í•´ í•©ì´ 1ì´ ë˜ë„ë¡ ë³´ì •.\n",
    "        \"\"\"\n",
    "        # í–‰ë™ ë²¡í„°ì˜ í•©ì´ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ 1e-8 ì¶”ê°€\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        # í˜„ì¬ ì‹œì ì˜ ì¢…ëª©ë³„ ìˆ˜ìµë¥ \n",
    "        current_return = self.returns[self.current_step]\n",
    "        # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ì€ ê° ì¢…ëª© ìˆ˜ìµë¥ ê³¼ íˆ¬ì ë¹„ì¤‘ì˜ ë‚´ì \n",
    "        reward = np.dot(weights, current_return)\n",
    "        self.current_step += 1  # ë‹¤ìŒ ë‚ ì§œë¡œ ì´ë™\n",
    "        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ íŒë‹¨: ëª¨ë“  ë°ì´í„° ì†Œì§„ ì‹œ ì¢…ë£Œ\n",
    "        done = self.current_step >= len(self.returns)\n",
    "        if not done:\n",
    "            next_state = self.returns[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.n_assets)  # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ, ì„ì˜ì˜ ìƒíƒœ ë°˜í™˜\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Gym í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ pandas DataFrameì„ numpy arrayë¡œ ë³€í™˜\n",
    "returns_np = returns_df.values\n",
    "\n",
    "##############################################\n",
    "# 4. Experience Replay Buffer êµ¬í˜„\n",
    "##############################################\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    ReplayBufferëŠ” ì—ì´ì „íŠ¸ì˜ ê²½í—˜(ìƒíƒœ, í–‰ë™, ë³´ìƒ, ë‹¤ìŒ ìƒíƒœ, ì¢…ë£Œ ì—¬ë¶€)ë¥¼ ì €ì¥í•˜ì—¬\n",
    "    ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ ì‹œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # ë²„í¼ì˜ ìµœëŒ€ ì €ì¥ í¬ê¸°\n",
    "        self.buffer = []         # ê²½í—˜ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "        self.position = 0        # ë‹¤ìŒ ì €ì¥í•  ì¸ë±ìŠ¤\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        ìƒˆë¡œìš´ ê²½í—˜ì„ ë²„í¼ì— ì €ì¥.\n",
    "        ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ìˆœí™˜ ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ ê²½í—˜ì„ ë®ì–´ì“´ë‹¤.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        # ì›í˜• ë²„í¼ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def minibatch_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        ì˜¤ë¦¬ì§€ë„ ì½”ë“œëŠ” í•¨ìˆ˜ëª…ì´ sampleì¸ë° ê¸°ì¡´ random.sample() ë©”ì„œë“œë‘ ë™ëª…ì´ë¼ í—·ê°ˆë ¤ì„œ ë‚´ê°€ ë°”ê¿ˆ\n",
    "        ì €ì¥ëœ ê²½í—˜ ì¤‘ ë¬´ì‘ìœ„ë¡œ ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§.\n",
    "        ë°˜í™˜ ê°’ì€ ê° í•­ëª©(ìƒíƒœ, í–‰ë™ ë“±)ì„ numpy arrayë¡œ ë¬¶ì–´ì¤Œ.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"í˜„ì¬ ë²„í¼ì— ì €ì¥ëœ ê²½í—˜ì˜ ê°œìˆ˜ ë°˜í™˜\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################################\n",
    "# 5. DDPG ëª¨ë¸ ì •ì˜: Actorì™€ Critic ë„¤íŠ¸ì›¤\n",
    "##############################################\n",
    "# Actor ë„¤íŠ¸ì›¤: ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„ ê° ìì‚°ì— íˆ¬ìí•  ë¹„ì¤‘(í™•ë¥  ë¶„í¬)ë¥¼ ì¶œë ¥\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # ì¶œë ¥ ê³„ì¸µ: ì€ë‹‰ì¸µì—ì„œ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘(ë¡œì§“ ê°’)ìœ¼ë¡œ ë³€í™˜\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        # softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì§“ì„ í™•ë¥  ë¶„í¬(í•©=1, ëª¨ë“  ì›ì†Œ 0 ì´ìƒ)ë¡œ ë³€í™˜\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3-softmax\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # ì¶œë ¥ ê³„ì¸µì„ í†µí•´ raw logits ìƒì„±\n",
    "        logits = self.fc3(x)\n",
    "        # softmax ì ìš©í•˜ì—¬ íˆ¬ì ë¹„ì¤‘ìœ¼ë¡œ ì •ê·œí™”\n",
    "        action = self.softmax(logits) # ë³´ì¥: ëª¨ë“  ì›ì†Œê°€ 0 ì´ìƒ, í•©=1\n",
    "        return action\n",
    "\n",
    "# Critic ë„¤íŠ¸ì›¤: ì£¼ì–´ì§„ ìƒíƒœì™€ í–‰ë™ì— ëŒ€í•´ Q-value(ì˜ˆìƒ ë¯¸ë˜ ë³´ìƒ)ë¥¼ ì¶œë ¥\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "##############################################\n",
    "# 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë„¤íŠ¸ì›¤ ì´ˆê¸°í™”\n",
    "##############################################\n",
    "state_dim = returns_np.shape[1]    # ìƒíƒœ ì°¨ì›: ìì‚°ì˜ ê°œìˆ˜\n",
    "action_dim = returns_np.shape[1]     # í–‰ë™ ì°¨ì›: í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ì˜ ê°œìˆ˜ (ê° ìì‚°ë§ˆë‹¤ í•˜ë‚˜ì”©)\n",
    "hidden_dim = 64                      # ì€ë‹‰ì¸µ ì°¨ì›\n",
    "\n",
    "# Actorì™€ Critic ë„¤íŠ¸ì›¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ ìƒì„±: ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ ì›ë³¸ ë„¤íŠ¸ì›¤ì˜ ëŠë¦° ì—…ë°ì´íŠ¸ ë²„ì „ ì‚¬ìš©\n",
    "target_actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "# ì´ˆê¸° íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ íŒŒë¼ë¯¸í„°ëŠ” ì›ë³¸ ë„¤íŠ¸ì›¤(ì—ì„œ copyí•˜ì—¬)ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizer ì„¤ì •: Actorì™€ Critic ë„¤íŠ¸ì›¤ ê°ê°ì— ëŒ€í•´ Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# replay_buffer ì´ˆê¸°í™”\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# í•™ìŠµ ê´€ë ¨ ê¸°íƒ€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "num_episodes = 50        # ì „ì²´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜\n",
    "batch_size = 64          # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§ í¬ê¸°\n",
    "gamma = 0.99\n",
    "tau = 0.005              # íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ soft update ë¹„ìœ¨ (ì›ë³¸ ë„¤íŠ¸ì›¤ì™€ íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë¹„ìœ¨)\n",
    "PRINT_INTERVAL = 5\n",
    "\n",
    "# íƒí—˜(exploration)ì„ ìœ„í•œ ë…¸ì´ì¦ˆ í•¨ìˆ˜: í–‰ë™ì— Gaussian ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ í›„ softmaxë¡œ ì •ê·œí™”\n",
    "# Ornstein-Uhlenbeck Noiseë¡œ í•œë‹¤ë©´ (Gaussian ë…¸ì´ì¦ˆ ëŒ€ë¹„) ì¥ë‹¨ì ì€?\n",
    "def add_noise(action, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "    noisy_action = action + noise\n",
    "    # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í–‰ë™ì„ ë‹¤ì‹œ softmax ì ìš©í•˜ì—¬ ì œì•½ ì¡°ê±´(ëª¨ë“  ì›ì†Œ 0 ì´ìƒ, í•©=1)ì„ ë§Œì¡±\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "##############################################\n",
    "# 7. DDPG í•™ìŠµ ë£¨í”„\n",
    "##############################################\n",
    "env = HistoricalPortfolioEnv(returns_np)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ í™˜ê²½ ì´ˆê¸°í™”\n",
    "    episode_reward = 0  # í•´ë‹¹ ì—í”¼ì†Œë“œì—ì„œ ëˆ„ì ëœ ë³´ìƒ\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # í˜„ì¬ ìƒíƒœë¥¼ tensorë¡œ ë³€í™˜í•˜ì—¬ Actor ë„¤íŠ¸ì›¤ì— ì…ë ¥\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        actor.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (dropout, batchnorm ë“±ì´ ìˆì„ ê²½ìš° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ)\n",
    "        with torch.no_grad():\n",
    "            # Actor ë„¤íŠ¸ì›¤ê°€ ì¶œë ¥í•œ íˆ¬ì ë¹„ì¤‘ (ì´ë¯¸ softmax ì ìš©ë¨)\n",
    "            action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        actor.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "        \n",
    "        # í•™ìŠµ ì‹œ, íƒí—˜ì„ ìœ„í•´ ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "        action = add_noise(action, noise_scale=0.1)\n",
    "        \n",
    "        # í™˜ê²½ì—ì„œ ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ íšë“\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # replay buffer ì— í˜„ì¬ ê²½í—˜ ì €ì¥\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # ì¶©ë¶„í•œ ê²½í—˜ì´ ìŒ“ì˜€ìœ¼ë©´, ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë„¤íŠ¸ì›¤ ì—…ë°ì´íŠ¸ ìˆ˜í–‰\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.minibatch_sample(batch_size)\n",
    "            \n",
    "            # numpy arrayë¥¼ torch tensorë¡œ ë³€í™˜\n",
    "            states_b = torch.FloatTensor(states_b)\n",
    "            actions_b = torch.FloatTensor(actions_b)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1)\n",
    "            next_states_b = torch.FloatTensor(next_states_b)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1)\n",
    "            \n",
    "            # Critic ì—…ë°ì´íŠ¸: ëª©í‘œ Q-value ê³„ì‚° ë° MSE ì†ì‹¤ ìµœì†Œí™”\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states_b) # íƒ€ê¹ƒ Actorë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ìƒíƒœì—ì„œì˜ í–‰ë™ ì˜ˆì¸¡\n",
    "                target_q = target_critic(next_states_b, next_actions) # íƒ€ê¹ƒ Criticìœ¼ë¡œ ë‹¤ìŒ ìƒíƒœì—ì„œì˜ Q-value ê³„ì‚°\n",
    "                # Bellman equationì— ë”°ë¥¸ ëª©í‘œ Q-value: í˜„ì¬ ë³´ìƒ + í• ì¸ìœ¨ * ë¯¸ë˜ Q-value * (ì¢…ë£Œ ì—¬ë¶€ ë°˜ì˜)\n",
    "                y = rewards_b + gamma * target_q * (1 - dones_b)\n",
    "            \n",
    "            # í˜„ì¬ Critic ë„¤íŠ¸ì›¤ì˜ Q-valueì™€ MSE-loss ê³„ì‚°\n",
    "            current_q = critic(states_b, actions_b)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            \n",
    "            # Critic ë„¤íŠ¸ì›¤ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor ì—…ë°ì´íŠ¸: ìƒíƒœì—ì„œì˜ í–‰ë™ì´ Critic ë„¤íŠ¸ì›¤ì—ì„œ ë†’ì€ Q-valueë¥¼ ìœ ë„í•˜ë„ë¡ ì—…ë°ì´íŠ¸\n",
    "            actor_loss = -critic(states_b, actor(states_b)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ soft update: ì›ë³¸ ë„¤íŠ¸ì›¤ íŒŒë¼ë¯¸í„°ì˜ ì¼ë¶€ë¥¼ íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ì— ë°˜ì˜í•˜ì—¬ ì²œì²œíˆ ì—…ë°ì´íŠ¸\n",
    "            \"\"\" ì˜¤ë¦¬ì§€ë„ ì½”ë“œì¸ë° ìˆœì„œë¥¼ ì•„ë˜ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ë³€ê²½í–ˆë‹¤\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \"\"\"\n",
    "            \n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    # ì—í”¼ì†Œë“œ ì§„í–‰ ì¤‘ PRINT_INTERVAL ì—í”¼ì†Œë“œë§ˆë‹¤ ê²°ê³¼ ì¶œë ¥\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f\"Episode {episode:3d} | Episode Reward: {episode_reward:.4f}\")\n",
    "\n",
    "print(\"DDPG í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "##############################################\n",
    "# 8. ë°±í…ŒìŠ¤íŒ…: í•™ìŠµëœ Actor ë„¤íŠ¸ì›¤ìœ¼ë¡œ ì „ì²´ ê¸°ê°„ì— ëŒ€í•´ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ì‚°ì¶œ\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í•™ìŠµëœ Actor ë„¤íŠ¸ì›¤ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë™ì•ˆì˜ ì¼ë³„ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ì„ ì‚°ì¶œ.\n",
    "    - ê²°ì •ë¡ ì  í–‰ë™ ì‚¬ìš©: íƒí—˜ ë…¸ì´ì¦ˆ ì—†ì´ Actorì˜ ì¶œë ¥(softmax ì ìš©ëœ íˆ¬ì ë¹„ì¤‘)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\n",
    "    \"\"\"\n",
    "    actor_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)  # ë°±í…ŒìŠ¤íŠ¸ìš© í™˜ê²½ ì´ˆê¸°í™”\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            # ê²°ì •ë¡ ì  í–‰ë™ ì‚°ì¶œ (íƒí—˜ ì—†ì´)\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        \n",
    "        next_state, reward, done, _ = env_bt.step(action) # í™˜ê²½ì—ì„œ ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ íšë“\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "# ë°±í…ŒìŠ¤íŠ¸: í•™ìŠµëœ DDPG Actorë¥¼ ì‚¬ìš©í•œ ì¼ë³„ ìˆ˜ìµë¥  ì‚°ì¶œ\n",
    "ddpg_daily_returns = backtest(actor, returns_np)\n",
    "# ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°: ë§¤ì¼ì˜ ìˆ˜ìµë¥ ì„ ëˆ„ì  ê³±í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ì„±ì¥ ê³¡ì„  ì‚°ì¶œ\n",
    "ddpg_cumulative = np.cumprod(1 + ddpg_daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 9. ë²¤ì¹˜ë§ˆí¬ ë°±í…ŒìŠ¤íŒ…: BM (ì‹œê°€ì´ì•¡ ë¹„ì¤‘ í¬íŠ¸í´ë¦¬ì˜¤)\n",
    "##############################################\n",
    "# ë²¤ì¹˜ë§ˆí¬ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì¼ë³„ ìˆ˜ìµë¥  ê³„ì‚°:\n",
    "# ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥ ì— ì‹œê°€ì´ì•¡ ë¹„ì¤‘ì„ ê³±í•œ í›„ í•©ì‚°\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ddpg_daily_returns):]\n",
    "# ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "# ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë‚ ì§œ (returns_dfì˜ index ì¤‘ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì‚¬ìš©)\n",
    "dates = returns_df.index[-len(ddpg_daily_returns):]\n",
    "\n",
    "##############################################\n",
    "# 10. ê²°ê³¼ Plot: BM vs DDPG\n",
    "##############################################\n",
    "plt.figure(figsize=(12,6))\n",
    "# ë²¤ì¹˜ë§ˆí¬ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ëˆ„ì  ìˆ˜ìµë¥  ê·¸ë˜í”„ (ì ì„  ìŠ¤íƒ€ì¼)\n",
    "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "# DDPG ì—ì´ì „íŠ¸ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ëˆ„ì  ìˆ˜ìµë¥  ê·¸ë˜í”„\n",
    "plt.plot(dates, ddpg_cumulative, label=\"DDPG Agent\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Cumulative returns\")\n",
    "plt.title(\"Backtesting: BM vs DDPG\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gaussian noise` ëŒ€ì‹  `Ornstein-Uhlenbeck noise`ë¥¼ ì‚¬ìš©í•˜ì.\n",
    "\n",
    "ì´ë¡ ì ìœ¼ë¡œëŠ”:\n",
    "\n",
    "`Gaussian ë…¸ì´ì¦ˆì˜ íŠ¹ì§•`\n",
    "- ë…ë¦½ì„±: ê° ì‹œê°„ ë‹¨ê³„ë§ˆë‹¤ ìƒì„±ë˜ëŠ” ë…¸ì´ì¦ˆ ê°’ì€ ì„œë¡œ ë…ë¦½ì ì´ë‹¤.\n",
    "- ê°„ë‹¨ì„±: êµ¬í˜„ê³¼ ì´í•´ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê³ , ì¶”ê°€ íŒŒë¼ë¯¸í„° ì—†ì´ ê¸°ë³¸ì ì¸ ì •ê·œë¶„í¬ì—ì„œ ë‚œìˆ˜ë¥¼ ìƒì„±í•œë‹¤.\n",
    "- ê¸‰ê²©í•œ ë³€í™”: ë§¤ ì‹œê°„ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ë˜ë¯€ë¡œ, í–‰ë™ì˜ ë³€í™”ê°€ ê°‘ì‘ìŠ¤ëŸ½ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "`OU ë…¸ì´ì¦ˆì˜ íŠ¹ì§•`\n",
    "- ì‹œê°„ ìƒê´€ì„±(`temporal correlations`): OU ë…¸ì´ì¦ˆëŠ” ì´ì „ ì‹œê°„ì˜ ìƒíƒœì— ì˜ì¡´í•˜ì—¬ ë…¸ì´ì¦ˆê°€ ìƒì„±ë˜ë¯€ë¡œ, ì—°ì†ëœ í–‰ë™ì— ëŒ€í•´ ë¶€ë“œëŸ½ê³  ì ì§„ì ì¸ ë³€í™”ë¥¼ ìœ ë„í•œë‹¤.\n",
    "- ë¬¼ë¦¬ì  í™˜ê²½ì— ì í•©: ë¬¼ë¦¬ì  ì‹œìŠ¤í…œì´ë‚˜ ê¸ˆìœµ ì‹œì¥ê³¼ ê°™ì´ ê´€ì„±ì´ ì¡´ì¬í•˜ëŠ” í™˜ê²½ì—ì„œëŠ” ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë³€í™”ë³´ë‹¤ëŠ” ì ì§„ì ì¸ ë³€í™”ê°€ ë” ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆë‹¤.\n",
    "- ì¶”ê°€ íŒŒë¼ë¯¸í„°: `mu, theta, sigma`ì™€ ê°™ì´ ëª‡ ê°€ì§€ ì¶”ê°€ íŒŒë¼ë¯¸í„°ê°€ ìˆì–´, ì´ë¥¼ ì ì ˆí•˜ê²Œ íŠœë‹í•´ì•¼ í•œë‹¤.\n",
    "\n",
    "ì¥ì  (OU ë…¸ì´ì¦ˆ ì‚¬ìš© ì‹œ)\n",
    "- ë¶€ë“œëŸ¬ìš´ í–‰ë™ ë³€í™”: ì‹œê°„ ìƒê´€ì„±ì´ ìˆìœ¼ë¯€ë¡œ, ì•¡ì…˜ì´ ê°‘ìê¸° íŠ€ì§€ ì•Šê³  ì—°ì†ì ì´ë‹¤.\n",
    "- í™˜ê²½ ì í•©ì„±: ë¬¼ë¦¬ì  ê´€ì„±ì´ ìˆê±°ë‚˜, ì—°ì†ì ì¸ ì œì–´ê°€ í•„ìš”í•œ í™˜ê²½ì—ì„œ íš¨ê³¼ì ì´ë‹¤.\n",
    "\n",
    "ë‹¨ì  (OU ë…¸ì´ì¦ˆ ì‚¬ìš© ì‹œ)\n",
    "- íŠœë‹ ë³µì¡ì„±: mu, theta, sigma ë“±ì˜ íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•˜ë©°, ë¶€ì ì ˆí•œ ê°’ ì„ íƒ ì‹œ íƒí—˜ì´ ê³¼ë„í•˜ê±°ë‚˜ ë¶€ì¡±í•  ìˆ˜ ìˆë‹¤.\n",
    "- ê³¼ë„í•œ ìƒê´€ì„±: ê²½ìš°ì— ë”°ë¼ ë„ˆë¬´ ë¶€ë“œëŸ¬ìš´ íƒí—˜ìœ¼ë¡œ ì¸í•´ ì¶©ë¶„í•œ ë‹¤ì–‘ì„±ì´ í™•ë³´ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "### (Recall)  Ornstein-Uhlenbeck í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "SDE:\n",
    "$$ dX_{t} = \\theta (\\mu - X_{t}) dt + \\sigma dW_{t} $$\n",
    "\n",
    "ì—¬ê¸°ì„œ, \n",
    "- $X_t$ = ì‹œê°„ ğ‘¡ì—ì„œì˜ ë…¸ì´ì¦ˆ ê°’\n",
    "- $\\mu$ = ì¥ê¸° í‰ê·  (long-run mean)ìœ¼ë¡œ, ë…¸ì´ì¦ˆê°€ ìˆ˜ë ´í•˜ë ¤ëŠ” ê°’\n",
    "- $\\theta$ = í‰ê·  íšŒê·€ ì†ë„ (mean reversion rate) $X_t$ ê°€ ğœ‡Î¼ë¡œ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ íšŒê·€í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•œë‹¤. ê°’ì´ í¬ë©´ ë…¸ì´ì¦ˆê°€ ë¹ ë¥´ê²Œ í‰ê· ìœ¼ë¡œ ëŒì•„ê°€, ê³¼ë„í•œ í¸ì°¨ë¥¼ ì–µì œí•œë‹¤.\n",
    "- ğœÏƒ = ë³€ë™ì„± (volatility) ë˜ëŠ” ë…¸ì´ì¦ˆ ê°•ë„, ë‚œìˆ˜ ì„±ë¶„ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. ê°’ì´ í¬ë©´ ë…¸ì´ì¦ˆì˜ ì§„í­ì´ ì»¤ì ¸ì„œ, ë” í° ë¬´ì‘ìœ„ ë³€ë™ì´ ë°œìƒí•œë‹¤.\n",
    "- $dW_{t}$= í‘œì¤€ Wiener í”„ë¡œì„¸ìŠ¤(ë¸Œë¼ìš´ ìš´ë™)ì—ì„œì˜ ë¯¸ì†Œ ë³€í™”ë¡œ, ğ‘(0, ğ‘‘ğ‘¡) N(0,dt)ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ì  ì„±ë¶„ì´ë‹¤.\n",
    "\n",
    "discrete time ì—ì„œì˜ ê·¼ì‚¬:\n",
    "ì‹¤ì œë¡œ êµ¬í˜„í•  ë•ŒëŠ” ë³´í†µ ì‹œê°„ ê°„ê²© ğ‘‘ğ‘¡=1ë¡œ ë‘ê³  ì•„ë˜ì™€ ê°™ì´ ê·¼ì‚¬í•œë‹¤:\n",
    "$$ X_{t+1} = X_{t} + \\theta ( \\mu - X_{t}) + \\sigma \\epsilon $$\n",
    "ì—¬ê¸°ì„œ ğœ–ÏµëŠ” ğ‘(0,1)ë¥¼ ë”°ë¥´ëŠ” ì •ê·œ ë¶„í¬ ë‚œìˆ˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise ëŒ€ì‹ ì— Ornstein-Uhlenbeck noiseë¡œ ë³€ê²½í•œ ì½”ë“œ\n",
    "# ì£¼ì„ì„ ì¶”ê°€í•œ ì½”ë“œ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "##############################################\n",
    "# í¬íŠ¸í´ë¦¬ì˜¤ì— ì‚¬ìš©í•  S&P500 ìƒìœ„ 20ì¢…ëª©ì˜ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "# ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ì„¤ì •: ì˜¤ëŠ˜ ë‚ ì§œì™€ 10ë…„ ì „ë¶€í„°ì˜ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# ê° ë‚ ì§œë³„ë¡œ ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥  ê³„ì‚° (ì „ì¼ ëŒ€ë¹„ ë³€í™”ìœ¨)\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. ë²¤ì¹˜ë§ˆí¬: ì‹œê°€ì´ì•¡ ë¹„ì¤‘ ë°°ë¶„ ê³„ì‚°\n",
    "##############################################\n",
    "# ê° ì¢…ëª©ì˜ ì‹œê°€ì´ì•¡ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë²¤ì¹˜ë§ˆí¬ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ë¹„ì¤‘ì„ ê³„ì‚°\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì„ì˜ì˜ ê°’ í• ë‹¹ (ë°ì´í„° ëˆ„ë½ ë°©ì§€)\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "# ê° ì¢…ëª©ì˜ ì‹œê°€ì´ì•¡ì„ ì „ì²´ ì‹œê°€ì´ì•¡ í•©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë²¤ì¹˜ë§ˆí¬ ë¹„ì¤‘ ê³„ì‚°\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "print(\"Benchmark weights (ì‹œê°€ì´ì•¡ ë¹„ì¤‘):\")\n",
    "for t, w in zip(tickers, benchmark_weights):\n",
    "    print(f\"{t}: {w:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# 3. í™˜ê²½(Environment) ì •ì˜\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ê³¼ê±° ì¼ë³„ ìˆ˜ìµë¥  ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” í™˜ê²½.\n",
    "    - ìƒíƒœ(state): í•´ë‹¹ ì¼ìì˜ ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥  (ë²¡í„°, shape: [n_assets])\n",
    "    - í–‰ë™(action): ê° ìì‚°ì— í• ë‹¹í•  íˆ¬ì ë¹„ì¤‘. ì—°ì†ì ì¸ ê°’ì´ë©° ëª¨ë“  ì›ì†ŒëŠ” 0 ì´ìƒ, í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½ë¨.\n",
    "      (Actor ë„¤íŠ¸ì›¤ì˜ softmax ì¶œë ¥ ê²°ê³¼ë¥¼ ì‚¬ìš©)\n",
    "    - ë³´ìƒ(reward): í•´ë‹¹ ì¼ì í¬íŠ¸í´ë¦¬ì˜¤ì˜ ìˆ˜ìµë¥ , ì¦‰ ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥ ê³¼ íˆ¬ì ë¹„ì¤‘ì˜ ë‚´ì (dot product)\n",
    "    \"\"\"\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns  # numpy array í˜•íƒœë¡œ ì¼ë³„ ìˆ˜ìµë¥  ë°ì´í„°, shape=(T, n_assets)\n",
    "        self.n_assets = returns.shape[1]  # ì¢…ëª©ì˜ ìˆ˜\n",
    "        self.current_step = 0  # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œì  ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "        # ìƒíƒœ ê³µê°„: ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥ , ë¬´í•œëŒ€ ë²”ìœ„ì˜ ì‹¤ìˆ˜ê°’ ê°€ëŠ¥\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # í–‰ë™ ê³µê°„: ê° ì¢…ëª©ì— í• ë‹¹í•  ë¹„ì¤‘, [0,1] ë²”ìœ„ì´ë©° ì‹¤ì œë¡œ softmaxë¥¼ í†µí•´ ì •ê·œí™”ë¨\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"ì—í”¼ì†Œë“œ ì´ˆê¸°í™”: ì‹œì‘ ë‹¨ê³„ë¡œ ëŒì•„ê°€ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ í–‰ë™(íˆ¬ì ë¹„ì¤‘)ì— ë”°ë¼ ë³´ìƒ(ë‹¹ì¼ ìˆ˜ìµë¥ )ì„ ê³„ì‚°í•˜ê³ , ë‹¤ìŒ ìƒíƒœë¡œ ì „í™˜.\n",
    "        - í–‰ë™ì´ ì •ê·œí™”ë˜ì§€ ì•Šì€ ê²½ìš°, softmaxì™€ ìœ ì‚¬í•˜ê²Œ ì •ê·œí™”í•´ í•©ì´ 1ì´ ë˜ë„ë¡ ë³´ì •.\n",
    "        \"\"\"\n",
    "        # í–‰ë™ ë²¡í„°ì˜ í•©ì´ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ 1e-8 ì¶”ê°€\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        # í˜„ì¬ ì‹œì ì˜ ì¢…ëª©ë³„ ìˆ˜ìµë¥ \n",
    "        current_return = self.returns[self.current_step]\n",
    "        # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ì€ ê° ì¢…ëª© ìˆ˜ìµë¥ ê³¼ íˆ¬ì ë¹„ì¤‘ì˜ ë‚´ì \n",
    "        reward = np.dot(weights, current_return)\n",
    "        self.current_step += 1  # ë‹¤ìŒ ë‚ ì§œë¡œ ì´ë™\n",
    "        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ íŒë‹¨: ëª¨ë“  ë°ì´í„° ì†Œì§„ ì‹œ ì¢…ë£Œ\n",
    "        done = self.current_step >= len(self.returns)\n",
    "        if not done:\n",
    "            next_state = self.returns[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.n_assets)  # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ, ì„ì˜ì˜ ìƒíƒœ ë°˜í™˜\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Gym í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ pandas DataFrameì„ numpy arrayë¡œ ë³€í™˜\n",
    "returns_np = returns_df.values\n",
    "\n",
    "##############################################\n",
    "# 4. Experience Replay Buffer êµ¬í˜„\n",
    "##############################################\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    ReplayBufferëŠ” ì—ì´ì „íŠ¸ì˜ ê²½í—˜(ìƒíƒœ, í–‰ë™, ë³´ìƒ, ë‹¤ìŒ ìƒíƒœ, ì¢…ë£Œ ì—¬ë¶€)ë¥¼ ì €ì¥í•˜ì—¬\n",
    "    ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ ì‹œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # ë²„í¼ì˜ ìµœëŒ€ ì €ì¥ í¬ê¸°\n",
    "        self.buffer = []         # ê²½í—˜ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "        self.position = 0        # ë‹¤ìŒ ì €ì¥í•  ì¸ë±ìŠ¤\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        ìƒˆë¡œìš´ ê²½í—˜ì„ ë²„í¼ì— ì €ì¥.\n",
    "        ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ìˆœí™˜ ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ ê²½í—˜ì„ ë®ì–´ì“´ë‹¤.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        # ì›í˜• ë²„í¼ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def minibatch_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        ì˜¤ë¦¬ì§€ë„ ì½”ë“œëŠ” í•¨ìˆ˜ëª…ì´ sampleì¸ë° ê¸°ì¡´ random.sample() ë©”ì„œë“œë‘ ë™ëª…ì´ë¼ í—·ê°ˆë ¤ì„œ ë‚´ê°€ ë°”ê¿ˆ\n",
    "        ì €ì¥ëœ ê²½í—˜ ì¤‘ ë¬´ì‘ìœ„ë¡œ ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§.\n",
    "        ë°˜í™˜ ê°’ì€ ê° í•­ëª©(ìƒíƒœ, í–‰ë™ ë“±)ì„ numpy arrayë¡œ ë¬¶ì–´ì¤Œ.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"í˜„ì¬ ë²„í¼ì— ì €ì¥ëœ ê²½í—˜ì˜ ê°œìˆ˜ ë°˜í™˜\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################################\n",
    "# 5. DDPG ëª¨ë¸ ì •ì˜: Actorì™€ Critic ë„¤íŠ¸ì›¤\n",
    "##############################################\n",
    "# Actor ë„¤íŠ¸ì›¤: ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„ ê° ìì‚°ì— íˆ¬ìí•  ë¹„ì¤‘(í™•ë¥  ë¶„í¬)ë¥¼ ì¶œë ¥\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # ì¶œë ¥ ê³„ì¸µ: ì€ë‹‰ì¸µì—ì„œ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘(ë¡œì§“ ê°’)ìœ¼ë¡œ ë³€í™˜\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        # softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì§“ì„ í™•ë¥  ë¶„í¬(í•©=1, ëª¨ë“  ì›ì†Œ 0 ì´ìƒ)ë¡œ ë³€í™˜\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3-softmax\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # ì¶œë ¥ ê³„ì¸µì„ í†µí•´ raw logits ìƒì„±\n",
    "        logits = self.fc3(x)\n",
    "        # softmax ì ìš©í•˜ì—¬ íˆ¬ì ë¹„ì¤‘ìœ¼ë¡œ ì •ê·œí™”\n",
    "        action = self.softmax(logits) # ë³´ì¥: ëª¨ë“  ì›ì†Œê°€ 0 ì´ìƒ, í•©=1\n",
    "        return action\n",
    "\n",
    "# Critic ë„¤íŠ¸ì›¤: ì£¼ì–´ì§„ ìƒíƒœì™€ í–‰ë™ì— ëŒ€í•´ Q-value(ì˜ˆìƒ ë¯¸ë˜ ë³´ìƒ)ë¥¼ ì¶œë ¥\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "##############################################\n",
    "# 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë„¤íŠ¸ì›¤ ì´ˆê¸°í™”\n",
    "##############################################\n",
    "state_dim = returns_np.shape[1]    # ìƒíƒœ ì°¨ì›: ìì‚°ì˜ ê°œìˆ˜\n",
    "action_dim = returns_np.shape[1]     # í–‰ë™ ì°¨ì›: í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ì˜ ê°œìˆ˜ (ê° ìì‚°ë§ˆë‹¤ í•˜ë‚˜ì”©)\n",
    "hidden_dim = 64                      # ì€ë‹‰ì¸µ ì°¨ì›\n",
    "\n",
    "# Actorì™€ Critic ë„¤íŠ¸ì›¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ ìƒì„±: ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ ì›ë³¸ ë„¤íŠ¸ì›¤ì˜ ëŠë¦° ì—…ë°ì´íŠ¸ ë²„ì „ ì‚¬ìš©\n",
    "target_actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "# ì´ˆê¸° íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ íŒŒë¼ë¯¸í„°ëŠ” ì›ë³¸ ë„¤íŠ¸ì›¤(ì—ì„œ copyí•˜ì—¬)ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizer ì„¤ì •: Actorì™€ Critic ë„¤íŠ¸ì›¤ ê°ê°ì— ëŒ€í•´ Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# replay_buffer ì´ˆê¸°í™”\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# í•™ìŠµ ê´€ë ¨ ê¸°íƒ€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "num_episodes = 50        # ì „ì²´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜\n",
    "batch_size = 64          # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§ í¬ê¸°\n",
    "gamma = 0.99\n",
    "tau = 0.005              # íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ soft update ë¹„ìœ¨ (ì›ë³¸ ë„¤íŠ¸ì›¤ì™€ íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë¹„ìœ¨)\n",
    "PRINT_INTERVAL = 5\n",
    "\n",
    "# íƒí—˜(exploration)ì„ ìœ„í•œ ë…¸ì´ì¦ˆ í•¨ìˆ˜: í–‰ë™ì— Gaussian ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ í›„ softmaxë¡œ ì •ê·œí™”\n",
    "# ê¸°ì¡´ì˜ add_noise í•¨ìˆ˜ ëŒ€ì‹  ì´ë¦„ì„ add_Gaussian_noiseë¡œ ë³€ê²½\n",
    "def add_Gaussian_noise(action, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "    noisy_action = action + noise\n",
    "    # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í–‰ë™ì„ ë‹¤ì‹œ softmax ì ìš©í•˜ì—¬ ì œì•½ ì¡°ê±´(ëª¨ë“  ì›ì†Œ 0 ì´ìƒ, í•©=1)ì„ ë§Œì¡±\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "# Ornstein-Uhlenbeck noiseë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ì •ì˜ (ë‚´ë¶€ ìƒíƒœ ìœ ì§€)\n",
    "# mu, theta, sigma ëŠ” ê°ê° ë…¸ì´ì¦ˆì˜ ì¥ê¸° í‰ê· , í‰ê·  íšŒê·€ ì†ë„(rate), ë³€ë™ì„±(volatility) ë˜ëŠ” ë…¸ì´ì¦ˆ ê°•ë„ì¸ë° ì–´ë–¤ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ëŠ”ê²Œ ì¢‹ì„ì§€?\n",
    "# íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def __call__(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = self.state + dx\n",
    "        return self.state\n",
    "\n",
    "# Ornstein-Uhlenbeck ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜\n",
    "def add_ornstein_uhlenbeck_noise(action, ou_noise):\n",
    "    noise = ou_noise()  # OU ë…¸ì´ì¦ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ë…¸ì´ì¦ˆ íšë“\n",
    "    noisy_action = action + noise\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "##############################################\n",
    "# 7. DDPG í•™ìŠµ ë£¨í”„\n",
    "##############################################\n",
    "env = HistoricalPortfolioEnv(returns_np)\n",
    "\n",
    "# DDPG í•™ìŠµ ë£¨í”„ ì‹œì‘ ì „, OU ë…¸ì´ì¦ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í–‰ë™ ì°¨ì›ì— ë§ì¶¤)\n",
    "ou_noise = OUNoise(action_dim, mu=0.0, theta=0.15, sigma=0.2)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ í™˜ê²½ ì´ˆê¸°í™”\n",
    "    episode_reward = 0  # í•´ë‹¹ ì—í”¼ì†Œë“œì—ì„œ ëˆ„ì ëœ ë³´ìƒ\n",
    "    done = False\n",
    "    ou_noise.reset()    # ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ OU ë…¸ì´ì¦ˆ ìƒíƒœ ì´ˆê¸°í™”\n",
    "    \n",
    "    while not done:\n",
    "        # í˜„ì¬ ìƒíƒœë¥¼ tensorë¡œ ë³€í™˜í•˜ì—¬ Actor ë„¤íŠ¸ì›¤ì— ì…ë ¥\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        actor.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (dropout, batchnorm ë“±ì´ ìˆì„ ê²½ìš° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ)\n",
    "        with torch.no_grad():\n",
    "            # Actor ë„¤íŠ¸ì›¤ê°€ ì¶œë ¥í•œ íˆ¬ì ë¹„ì¤‘ (ì´ë¯¸ softmax ì ìš©ë¨)\n",
    "            action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        actor.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "        \n",
    "        \"\"\"\n",
    "        # í•™ìŠµ ì‹œ, íƒí—˜ì„ ìœ„í•´ Gaussian ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "        action = add_Gaussian_noise(action, noise_scale=0.1)\n",
    "        \"\"\"\n",
    "        # ê¸°ì¡´ Gaussian ë…¸ì´ì¦ˆ ëŒ€ì‹  Ornstein-Uhlenbeck ë…¸ì´ì¦ˆ ì ìš©\n",
    "        action = add_ornstein_uhlenbeck_noise(action, ou_noise)\n",
    "        \n",
    "        # í™˜ê²½ì—ì„œ ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ íšë“\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # replay buffer ì— í˜„ì¬ ê²½í—˜ ì €ì¥\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # ì¶©ë¶„í•œ ê²½í—˜ì´ ìŒ“ì˜€ìœ¼ë©´, ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë„¤íŠ¸ì›¤ ì—…ë°ì´íŠ¸ ìˆ˜í–‰\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.minibatch_sample(batch_size)\n",
    "            \n",
    "            # numpy arrayë¥¼ torch tensorë¡œ ë³€í™˜\n",
    "            states_b = torch.FloatTensor(states_b)\n",
    "            actions_b = torch.FloatTensor(actions_b)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1)\n",
    "            next_states_b = torch.FloatTensor(next_states_b)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1)\n",
    "            \n",
    "            # Critic ì—…ë°ì´íŠ¸: ëª©í‘œ Q-value ê³„ì‚° ë° MSE ì†ì‹¤ ìµœì†Œí™”\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states_b) # íƒ€ê¹ƒ Actorë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ìƒíƒœì—ì„œì˜ í–‰ë™ ì˜ˆì¸¡\n",
    "                target_q = target_critic(next_states_b, next_actions) # íƒ€ê¹ƒ Criticìœ¼ë¡œ ë‹¤ìŒ ìƒíƒœì—ì„œì˜ Q-value ê³„ì‚°\n",
    "                # Bellman equationì— ë”°ë¥¸ ëª©í‘œ Q-value: í˜„ì¬ ë³´ìƒ + í• ì¸ìœ¨ * ë¯¸ë˜ Q-value * (ì¢…ë£Œ ì—¬ë¶€ ë°˜ì˜)\n",
    "                y = rewards_b + gamma * target_q * (1 - dones_b)\n",
    "            \n",
    "            # í˜„ì¬ Critic ë„¤íŠ¸ì›¤ì˜ Q-valueì™€ MSE-loss ê³„ì‚°\n",
    "            current_q = critic(states_b, actions_b)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            \n",
    "            # Critic ë„¤íŠ¸ì›¤ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor ì—…ë°ì´íŠ¸: ìƒíƒœì—ì„œì˜ í–‰ë™ì´ Critic ë„¤íŠ¸ì›¤ì—ì„œ ë†’ì€ Q-valueë¥¼ ìœ ë„í•˜ë„ë¡ ì—…ë°ì´íŠ¸\n",
    "            actor_loss = -critic(states_b, actor(states_b)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ soft update: ì›ë³¸ ë„¤íŠ¸ì›¤ íŒŒë¼ë¯¸í„°ì˜ ì¼ë¶€ë¥¼ íƒ€ê¹ƒ ë„¤íŠ¸ì›¤ì— ë°˜ì˜í•˜ì—¬ ì²œì²œíˆ ì—…ë°ì´íŠ¸\n",
    "            \"\"\" ì˜¤ë¦¬ì§€ë„ ì½”ë“œì¸ë° ìˆœì„œë¥¼ ì•„ë˜ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ë³€ê²½í–ˆë‹¤\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \"\"\"\n",
    "            \n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    # ì—í”¼ì†Œë“œ ì§„í–‰ ì¤‘ PRINT_INTERVAL ì—í”¼ì†Œë“œë§ˆë‹¤ ê²°ê³¼ ì¶œë ¥\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f\"Episode {episode:3d} | Episode Reward: {episode_reward:.4f}\")\n",
    "\n",
    "print(\"DDPG í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "##############################################\n",
    "# 8. ë°±í…ŒìŠ¤íŒ…: í•™ìŠµëœ Actor ë„¤íŠ¸ì›¤ìœ¼ë¡œ ì „ì²´ ê¸°ê°„ì— ëŒ€í•´ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ì‚°ì¶œ\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í•™ìŠµëœ Actor ë„¤íŠ¸ì›¤ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë™ì•ˆì˜ ì¼ë³„ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ì„ ì‚°ì¶œ.\n",
    "    - ê²°ì •ë¡ ì  í–‰ë™ ì‚¬ìš©: íƒí—˜ ë…¸ì´ì¦ˆ ì—†ì´ Actorì˜ ì¶œë ¥(softmax ì ìš©ëœ íˆ¬ì ë¹„ì¤‘)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\n",
    "    \"\"\"\n",
    "    actor_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)  # ë°±í…ŒìŠ¤íŠ¸ìš© í™˜ê²½ ì´ˆê¸°í™”\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            # ê²°ì •ë¡ ì  í–‰ë™ ì‚°ì¶œ (íƒí—˜ ì—†ì´)\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        \n",
    "        next_state, reward, done, _ = env_bt.step(action) # í™˜ê²½ì—ì„œ ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ íšë“\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "# ë°±í…ŒìŠ¤íŠ¸: í•™ìŠµëœ DDPG Actorë¥¼ ì‚¬ìš©í•œ ì¼ë³„ ìˆ˜ìµë¥  ì‚°ì¶œ\n",
    "ddpg_daily_returns = backtest(actor, returns_np)\n",
    "# ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°: ë§¤ì¼ì˜ ìˆ˜ìµë¥ ì„ ëˆ„ì  ê³±í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ì„±ì¥ ê³¡ì„  ì‚°ì¶œ\n",
    "ddpg_cumulative = np.cumprod(1 + ddpg_daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 9. ë²¤ì¹˜ë§ˆí¬ ë°±í…ŒìŠ¤íŒ…: BM (ì‹œê°€ì´ì•¡ ë¹„ì¤‘ í¬íŠ¸í´ë¦¬ì˜¤)\n",
    "##############################################\n",
    "# ë²¤ì¹˜ë§ˆí¬ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì¼ë³„ ìˆ˜ìµë¥  ê³„ì‚°:\n",
    "# ê° ì¢…ëª©ì˜ ìˆ˜ìµë¥ ì— ì‹œê°€ì´ì•¡ ë¹„ì¤‘ì„ ê³±í•œ í›„ í•©ì‚°\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ddpg_daily_returns):]\n",
    "# ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "# ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë‚ ì§œ (returns_dfì˜ index ì¤‘ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì‚¬ìš©)\n",
    "dates = returns_df.index[-len(ddpg_daily_returns):]\n",
    "\n",
    "##############################################\n",
    "# 10. ê²°ê³¼ Plot: BM vs DDPG\n",
    "##############################################\n",
    "plt.figure(figsize=(12,6))\n",
    "# ë²¤ì¹˜ë§ˆí¬ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ëˆ„ì  ìˆ˜ìµë¥  ê·¸ë˜í”„ (ì ì„  ìŠ¤íƒ€ì¼)\n",
    "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "# DDPG ì—ì´ì „íŠ¸ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ëˆ„ì  ìˆ˜ìµë¥  ê·¸ë˜í”„\n",
    "plt.plot(dates, ddpg_cumulative, label=\"DDPG Agent\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Cumulative returns\")\n",
    "plt.title(\"Backtesting: BM vs DDPG\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
