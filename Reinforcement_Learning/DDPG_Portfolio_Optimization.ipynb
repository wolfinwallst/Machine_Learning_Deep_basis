{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG는 연속적 행동 공간에서 결정론적 정책을 학습하는 알고리즘입니다.\n",
    "- 여기서는 포트폴리오의 제약(각 자산 비중이 0 이상, 총합 1)을 만족하기 위해 Actor 네트워크의 출력을 softmax를 통해 정규화합니다.\n",
    "- 탐험을 위해 학습 시에는 Actor의 출력에 Gaussian 노이즈를 추가합니다.\n",
    "- 경험 재현(Replay Buffer)과 타깃 네트워크(target network)의 soft update를 사용합니다.\n",
    "\n",
    "DDPG 에이전트 구성:\n",
    "\n",
    "- Actor 네트워크: 상태를 입력받아 raw logits를 출력한 후 softmax를 적용하여, 각 자산의 투자 비중(0 이상, 합=1)을 생성합니다.\n",
    "- Critic 네트워크: 상태와 행동을 입력받아 Q-value를 출력합니다.\n",
    "- 타깃 네트워크와 경험 재현(Replay Buffer)을 사용하며, 학습 중에는 탐험을 위해 Gaussian 노이즈를 추가합니다.\n",
    "\n",
    "학습 및 업데이트:\n",
    "\n",
    "- Critic은 TD 목표와 현재 Q값 간의 MSE 손실로 업데이트합니다.\n",
    "- Actor는 Critic의 Q값을 최대화하도록 업데이트합니다.\n",
    "- 타깃 네트워크는 소프트 업데이트 방식(𝜏 τ)으로 주기적으로 갱신됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC 간단한 포트폴리오 최적화 via DDPG\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. 데이터 다운로드 및 전처리\n",
    "##############################################\n",
    "# S&P500 상위 20종목 (예시 티커)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "# 10년 전부터 오늘까지의 데이터\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 일별 수익률 계산\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. 벤치마크: 시가총액 비중 배분 계산\n",
    "##############################################\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "print(\"Benchmark weights (시가총액 비중):\")\n",
    "for t, w in zip(tickers, benchmark_weights):\n",
    "    print(f\"{t}: {w:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# 3. 환경(Environment) 정의\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    과거 일별 수익률 데이터를 순차적으로 제공하는 환경.\n",
    "    - 상태(state): 해당일의 각 종목 수익률 (vector, shape: [n_assets])\n",
    "    - 행동(action): 각 자산에 할당할 투자 비중 (연속적, 단, 모든 원소 ≥0, 합=1)\n",
    "    - 보상(reward): 선택한 포트폴리오의 당일 수익률 (각 자산 수익률과 배분 비중의 내적)\n",
    "    \"\"\"\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns  # numpy array, shape=(T, n_assets)\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.current_step = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # 행동은 연속적 벡터 (실제 action은 Actor의 softmax 출력을 사용)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 혹시라도 action이 정규화되어 있지 않다면 보정\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        current_return = self.returns[self.current_step]\n",
    "        reward = np.dot(weights, current_return)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.returns)\n",
    "        if not done:\n",
    "            next_state = self.returns[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.n_assets)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# numpy array로 변환\n",
    "returns_np = returns_df.values\n",
    "\n",
    "##############################################\n",
    "# 4. Experience Replay Buffer 구현\n",
    "##############################################\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################################\n",
    "# 5. DDPG 모델 정의: Actor와 Critic 네트워크\n",
    "##############################################\n",
    "# Actor: 상태를 받아 raw logits를 출력하고 softmax를 통해 포트폴리오 비중 (합=1) 생성\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3-softmax\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        action = self.softmax(logits)  # 보장: 모든 원소가 0 이상, 합=1\n",
    "        return action\n",
    "\n",
    "# Critic: 상태와 행동을 받아 Q-value 출력\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "##############################################\n",
    "# 6. 하이퍼파라미터 및 네트워크 초기화\n",
    "##############################################\n",
    "state_dim = returns_np.shape[1]    # n_assets\n",
    "action_dim = returns_np.shape[1]     # 포트폴리오 비중 차원\n",
    "hidden_dim = 64\n",
    "\n",
    "# Actor와 Critic 네트워크\n",
    "actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# 타깃 네트워크\n",
    "target_actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizer\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# 기타 하이퍼파라미터\n",
    "num_episodes = 50\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 타깃 네트워크 soft update 비율\n",
    "\n",
    "# 탐험을 위한 노이즈 (Gaussian noise)\n",
    "def add_noise(action, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "    noisy_action = action + noise\n",
    "    # softmax로 정규화해서 제약 조건(합=1, 각 원소 ≥ 0) 유지\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "##############################################\n",
    "# 7. DDPG 학습 루프\n",
    "##############################################\n",
    "env = HistoricalPortfolioEnv(returns_np)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        actor.train()\n",
    "        \n",
    "        # 탐험을 위해 노이즈 추가 (학습 시)\n",
    "        action = add_noise(action, noise_scale=0.1)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # 미니배치 업데이트\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            states_b = torch.FloatTensor(states_b)\n",
    "            actions_b = torch.FloatTensor(actions_b)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1)\n",
    "            next_states_b = torch.FloatTensor(next_states_b)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1)\n",
    "            \n",
    "            # Critic 업데이트\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states_b)\n",
    "                target_q = target_critic(next_states_b, next_actions)\n",
    "                y = rewards_b + gamma * target_q * (1 - dones_b)\n",
    "            \n",
    "            current_q = critic(states_b, actions_b)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor 업데이트: maximize Q(s, actor(s))\n",
    "            actor_loss = -critic(states_b, actor(states_b)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # 타깃 네트워크 soft update\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Episode {episode:3d} | Episode Reward: {episode_reward:.4f}\")\n",
    "\n",
    "print(\"DDPG 학습 완료\")\n",
    "\n",
    "##############################################\n",
    "# 8. 백테스팅: 학습된 Actor 네트워크로 전체 기간에 대해 포트폴리오 수익률 산출\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    actor_model.eval()\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        # 결정론적 행동: 노이즈 없이 actor의 출력 사용 (이미 softmax 적용됨)\n",
    "        next_state, reward, done, _ = env_bt.step(action)\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "ddpg_daily_returns = backtest(actor, returns_np)\n",
    "ddpg_cumulative = np.cumprod(1 + ddpg_daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 9. 벤치마크 백테스팅: BM (시가총액 비중 포트폴리오)\n",
    "##############################################\n",
    "# 벤치마크의 일별 수익률: 각 종목 수익률에 벤치마크 비중 곱합\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ddpg_daily_returns):]\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "# 백테스트 기간 날짜 (returns_df의 index 사용)\n",
    "dates = returns_df.index[-len(ddpg_daily_returns):]\n",
    "\n",
    "##############################################\n",
    "# 10. 결과 Plot: BM vs DDPG\n",
    "##############################################\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "plt.plot(dates, ddpg_cumulative, label=\"DDPG Agent\")\n",
    "plt.xlabel(\"날짜\")\n",
    "plt.ylabel(\"누적 수익률\")\n",
    "plt.title(\"Backtesting: BM vs DDPG\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주석을 추가한 코드\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. 데이터 다운로드 및 전처리\n",
    "##############################################\n",
    "# 포트폴리오에 사용할 S&P500 상위 20종목의 티커 리스트 (예시)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "# 시작일과 종료일을 설정: 오늘 날짜와 10년 전부터의 데이터를 다운로드\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 각 날짜별로 각 종목의 수익률 계산 (전일 대비 변화율)\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. 벤치마크: 시가총액 비중 배분 계산\n",
    "##############################################\n",
    "# 각 종목의 시가총액 정보를 이용하여 벤치마크 포트폴리오의 비중을 계산\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시에도 임의의 값 할당 (데이터 누락 방지)\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "# 각 종목의 시가총액을 전체 시가총액 합으로 나누어 벤치마크 비중 계산\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "print(\"Benchmark weights (시가총액 비중):\")\n",
    "for t, w in zip(tickers, benchmark_weights):\n",
    "    print(f\"{t}: {w:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# 3. 환경(Environment) 정의\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    과거 일별 수익률 데이터를 순차적으로 제공하는 환경.\n",
    "    - 상태(state): 해당 일자의 각 종목의 수익률 (벡터, shape: [n_assets])\n",
    "    - 행동(action): 각 자산에 할당할 투자 비중. 연속적인 값이며 모든 원소는 0 이상, 합이 1이 되도록 제약됨.\n",
    "      (Actor 네트웤의 softmax 출력 결과를 사용)\n",
    "    - 보상(reward): 해당 일자 포트폴리오의 수익률, 즉 각 종목의 수익률과 투자 비중의 내적(dot product)\n",
    "    \"\"\"\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns  # numpy array 형태로 일별 수익률 데이터, shape=(T, n_assets)\n",
    "        self.n_assets = returns.shape[1]  # 종목의 수\n",
    "        self.current_step = 0  # 에피소드 시작 시점 인덱스 초기화\n",
    "        # 상태 공간: 각 종목의 수익률, 무한대 범위의 실수값 가능\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # 행동 공간: 각 종목에 할당할 비중, [0,1] 범위이며 실제로 softmax를 통해 정규화됨\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"에피소드 초기화: 시작 단계로 돌아가 현재 상태를 반환\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        주어진 행동(투자 비중)에 따라 보상(당일 수익률)을 계산하고, 다음 상태로 전환.\n",
    "        - 행동이 정규화되지 않은 경우, softmax와 유사하게 정규화해 합이 1이 되도록 보정.\n",
    "        \"\"\"\n",
    "        # 행동 벡터의 합이 0이 되는 것을 방지하기 위해 작은 값 1e-8 추가\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        # 현재 시점의 종목별 수익률\n",
    "        current_return = self.returns[self.current_step]\n",
    "        # 포트폴리오 수익률은 각 종목 수익률과 투자 비중의 내적\n",
    "        reward = np.dot(weights, current_return)\n",
    "        self.current_step += 1  # 다음 날짜로 이동\n",
    "        # 에피소드 종료 여부 판단: 모든 데이터 소진 시 종료\n",
    "        done = self.current_step >= len(self.returns)\n",
    "        if not done:\n",
    "            next_state = self.returns[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.n_assets)  # 에피소드 종료 시, 임의의 상태 반환\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Gym 환경에서 사용할 수 있도록 pandas DataFrame을 numpy array로 변환\n",
    "returns_np = returns_df.values\n",
    "\n",
    "##############################################\n",
    "# 4. Experience Replay Buffer 구현\n",
    "##############################################\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    ReplayBuffer는 에이전트의 경험(상태, 행동, 보상, 다음 상태, 종료 여부)를 저장하여\n",
    "    미니배치 학습 시 무작위로 샘플링할 수 있도록 지원한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # 버퍼의 최대 저장 크기\n",
    "        self.buffer = []         # 경험 저장 리스트\n",
    "        self.position = 0        # 다음 저장할 인덱스\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        새로운 경험을 버퍼에 저장.\n",
    "        버퍼가 가득 차면 순환 방식으로 기존 경험을 덮어쓴다.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        # 원형 버퍼를 위한 인덱스 업데이트\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def minibatch_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        오리지널 코드는 함수명이 sample인데 기존 random.sample() 메서드랑 동명이라 헷갈려서 내가 바꿈\n",
    "        저장된 경험 중 무작위로 미니배치 샘플링.\n",
    "        반환 값은 각 항목(상태, 행동 등)을 numpy array로 묶어줌.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"현재 버퍼에 저장된 경험의 개수 반환\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################################\n",
    "# 5. DDPG 모델 정의: Actor와 Critic 네트웤\n",
    "##############################################\n",
    "# Actor 네트웤: 상태를 입력받아 각 자산에 투자할 비중(확률 분포)를 출력\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # 출력 계층: 은닉층에서 포트폴리오 비중(로짓 값)으로 변환\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        # softmax를 사용하여 로짓을 확률 분포(합=1, 모든 원소 0 이상)로 변환\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3-softmax\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 출력 계층을 통해 raw logits 생성\n",
    "        logits = self.fc3(x)\n",
    "        # softmax 적용하여 투자 비중으로 정규화\n",
    "        action = self.softmax(logits) # 보장: 모든 원소가 0 이상, 합=1\n",
    "        return action\n",
    "\n",
    "# Critic 네트웤: 주어진 상태와 행동에 대해 Q-value(예상 미래 보상)를 출력\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "##############################################\n",
    "# 6. 하이퍼파라미터 및 네트웤 초기화\n",
    "##############################################\n",
    "state_dim = returns_np.shape[1]    # 상태 차원: 자산의 개수\n",
    "action_dim = returns_np.shape[1]     # 행동 차원: 포트폴리오 비중의 개수 (각 자산마다 하나씩)\n",
    "hidden_dim = 64                      # 은닉층 차원\n",
    "\n",
    "# Actor와 Critic 네트웤 인스턴스 생성\n",
    "actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# 타깃 네트웤 생성: 안정적인 학습을 위해 원본 네트웤의 느린 업데이트 버전 사용\n",
    "target_actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "# 초기 타깃 네트웤 파라미터는 원본 네트웤(에서 copy하여)과 동일하게 설정\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizer 설정: Actor와 Critic 네트웤 각각에 대해 Adam 옵티마이저 사용\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# replay_buffer 초기화\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# 학습 관련 기타 하이퍼파라미터 설정\n",
    "num_episodes = 50        # 전체 학습 에피소드 수\n",
    "batch_size = 64          # 미니배치 샘플링 크기\n",
    "gamma = 0.99\n",
    "tau = 0.005              # 타깃 네트웤 soft update 비율 (원본 네트웤와 타깃 네트웤의 파라미터 업데이트 비율)\n",
    "PRINT_INTERVAL = 5\n",
    "\n",
    "# 탐험(exploration)을 위한 노이즈 함수: 행동에 Gaussian 노이즈를 추가한 후 softmax로 정규화\n",
    "# Ornstein-Uhlenbeck Noise로 한다면 (Gaussian 노이즈 대비) 장단점은?\n",
    "def add_noise(action, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "    noisy_action = action + noise\n",
    "    # 노이즈가 추가된 행동을 다시 softmax 적용하여 제약 조건(모든 원소 0 이상, 합=1)을 만족\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "##############################################\n",
    "# 7. DDPG 학습 루프\n",
    "##############################################\n",
    "env = HistoricalPortfolioEnv(returns_np)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # 에피소드 시작 시 환경 초기화\n",
    "    episode_reward = 0  # 해당 에피소드에서 누적된 보상\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 현재 상태를 tensor로 변환하여 Actor 네트웤에 입력\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        actor.eval()  # 평가 모드로 전환 (dropout, batchnorm 등이 있을 경우 영향을 줄 수 있음)\n",
    "        with torch.no_grad():\n",
    "            # Actor 네트웤가 출력한 투자 비중 (이미 softmax 적용됨)\n",
    "            action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        actor.train()  # 다시 학습 모드로 전환\n",
    "        \n",
    "        # 학습 시, 탐험을 위해 노이즈 추가\n",
    "        action = add_noise(action, noise_scale=0.1)\n",
    "        \n",
    "        # 환경에서 다음 상태, 보상, 종료 여부 획득\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # replay buffer 에 현재 경험 저장\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # 충분한 경험이 쌓였으면, 미니배치로 네트웤 업데이트 수행\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # 미니배치 샘플링\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.minibatch_sample(batch_size)\n",
    "            \n",
    "            # numpy array를 torch tensor로 변환\n",
    "            states_b = torch.FloatTensor(states_b)\n",
    "            actions_b = torch.FloatTensor(actions_b)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1)\n",
    "            next_states_b = torch.FloatTensor(next_states_b)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1)\n",
    "            \n",
    "            # Critic 업데이트: 목표 Q-value 계산 및 MSE 손실 최소화\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states_b) # 타깃 Actor를 사용해 다음 상태에서의 행동 예측\n",
    "                target_q = target_critic(next_states_b, next_actions) # 타깃 Critic으로 다음 상태에서의 Q-value 계산\n",
    "                # Bellman equation에 따른 목표 Q-value: 현재 보상 + 할인율 * 미래 Q-value * (종료 여부 반영)\n",
    "                y = rewards_b + gamma * target_q * (1 - dones_b)\n",
    "            \n",
    "            # 현재 Critic 네트웤의 Q-value와 MSE-loss 계산\n",
    "            current_q = critic(states_b, actions_b)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            \n",
    "            # Critic 네트웤 파라미터 업데이트\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor 업데이트: 상태에서의 행동이 Critic 네트웤에서 높은 Q-value를 유도하도록 업데이트\n",
    "            actor_loss = -critic(states_b, actor(states_b)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # 타깃 네트웤 soft update: 원본 네트웤 파라미터의 일부를 타깃 네트웤에 반영하여 천천히 업데이트\n",
    "            \"\"\" 오리지널 코드인데 순서를 아래처럼 깔끔하게 변경했다\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \"\"\"\n",
    "            \n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    # 에피소드 진행 중 PRINT_INTERVAL 에피소드마다 결과 출력\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f\"Episode {episode:3d} | Episode Reward: {episode_reward:.4f}\")\n",
    "\n",
    "print(\"DDPG 학습 완료\")\n",
    "\n",
    "##############################################\n",
    "# 8. 백테스팅: 학습된 Actor 네트웤으로 전체 기간에 대해 포트폴리오 수익률 산출\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    \"\"\"\n",
    "    주어진 학습된 Actor 네트웤을 사용하여 전체 백테스트 기간 동안의 일별 포트폴리오 수익률을 산출.\n",
    "    - 결정론적 행동 사용: 탐험 노이즈 없이 Actor의 출력(softmax 적용된 투자 비중)을 그대로 사용.\n",
    "    \"\"\"\n",
    "    actor_model.eval()  # 평가 모드로 전환\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)  # 백테스트용 환경 초기화\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            # 결정론적 행동 산출 (탐험 없이)\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        \n",
    "        next_state, reward, done, _ = env_bt.step(action) # 환경에서 다음 상태, 보상, 종료 여부 획득\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "# 백테스트: 학습된 DDPG Actor를 사용한 일별 수익률 산출\n",
    "ddpg_daily_returns = backtest(actor, returns_np)\n",
    "# 누적 수익률 계산: 매일의 수익률을 누적 곱하여 포트폴리오 성장 곡선 산출\n",
    "ddpg_cumulative = np.cumprod(1 + ddpg_daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 9. 벤치마크 백테스팅: BM (시가총액 비중 포트폴리오)\n",
    "##############################################\n",
    "# 벤치마크 포트폴리오의 일별 수익률 계산:\n",
    "# 각 종목의 수익률에 시가총액 비중을 곱한 후 합산\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ddpg_daily_returns):]\n",
    "# 누적 수익률 계산\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "# 백테스트 기간 날짜 (returns_df의 index 중 마지막 부분을 사용)\n",
    "dates = returns_df.index[-len(ddpg_daily_returns):]\n",
    "\n",
    "##############################################\n",
    "# 10. 결과 Plot: BM vs DDPG\n",
    "##############################################\n",
    "plt.figure(figsize=(12,6))\n",
    "# 벤치마크 포트폴리오의 누적 수익률 그래프 (점선 스타일)\n",
    "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "# DDPG 에이전트 포트폴리오의 누적 수익률 그래프\n",
    "plt.plot(dates, ddpg_cumulative, label=\"DDPG Agent\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Cumulative returns\")\n",
    "plt.title(\"Backtesting: BM vs DDPG\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gaussian noise` 대신 `Ornstein-Uhlenbeck noise`를 사용하자.\n",
    "\n",
    "이론적으로는:\n",
    "\n",
    "`Gaussian 노이즈의 특징`\n",
    "- 독립성: 각 시간 단계마다 생성되는 노이즈 값은 서로 독립적이다.\n",
    "- 간단성: 구현과 이해가 상대적으로 쉽고, 추가 파라미터 없이 기본적인 정규분포에서 난수를 생성한다.\n",
    "- 급격한 변화: 매 시간마다 독립적으로 노이즈가 추가되므로, 행동의 변화가 갑작스럽게 나타날 수 있다.\n",
    "\n",
    "`OU 노이즈의 특징`\n",
    "- 시간 상관성(`temporal correlations`): OU 노이즈는 이전 시간의 상태에 의존하여 노이즈가 생성되므로, 연속된 행동에 대해 부드럽고 점진적인 변화를 유도한다.\n",
    "- 물리적 환경에 적합: 물리적 시스템이나 금융 시장과 같이 관성이 존재하는 환경에서는 갑작스러운 변화보다는 점진적인 변화가 더 자연스러울 수 있다.\n",
    "- 추가 파라미터: `mu, theta, sigma`와 같이 몇 가지 추가 파라미터가 있어, 이를 적절하게 튜닝해야 한다.\n",
    "\n",
    "장점 (OU 노이즈 사용 시)\n",
    "- 부드러운 행동 변화: 시간 상관성이 있으므로, 액션이 갑자기 튀지 않고 연속적이다.\n",
    "- 환경 적합성: 물리적 관성이 있거나, 연속적인 제어가 필요한 환경에서 효과적이다.\n",
    "\n",
    "단점 (OU 노이즈 사용 시)\n",
    "- 튜닝 복잡성: mu, theta, sigma 등의 파라미터 튜닝이 필요하며, 부적절한 값 선택 시 탐험이 과도하거나 부족할 수 있다.\n",
    "- 과도한 상관성: 경우에 따라 너무 부드러운 탐험으로 인해 충분한 다양성이 확보되지 않을 수 있다.\n",
    "\n",
    "### (Recall)  Ornstein-Uhlenbeck 프로세스\n",
    "\n",
    "SDE:\n",
    "$$ dX_{t} = \\theta (\\mu - X_{t}) dt + \\sigma dW_{t} $$\n",
    "\n",
    "여기서, \n",
    "- $X_t$ = 시간 𝑡에서의 노이즈 값\n",
    "- $\\mu$ = 장기 평균 (long-run mean)으로, 노이즈가 수렴하려는 값\n",
    "- $\\theta$ = 평균 회귀 속도 (mean reversion rate) $X_t$ 가 𝜇μ로 얼마나 빨리 회귀하는지를 결정한다. 값이 크면 노이즈가 빠르게 평균으로 돌아가, 과도한 편차를 억제한다.\n",
    "- 𝜎σ = 변동성 (volatility) 또는 노이즈 강도, 난수 성분의 크기를 결정한다. 값이 크면 노이즈의 진폭이 커져서, 더 큰 무작위 변동이 발생한다.\n",
    "- $dW_{t}$= 표준 Wiener 프로세스(브라운 운동)에서의 미소 변화로, 𝑁(0, 𝑑𝑡) N(0,dt)를 따르는 확률적 성분이다.\n",
    "\n",
    "discrete time 에서의 근사:\n",
    "실제로 구현할 때는 보통 시간 간격 𝑑𝑡=1로 두고 아래와 같이 근사한다:\n",
    "$$ X_{t+1} = X_{t} + \\theta ( \\mu - X_{t}) + \\sigma \\epsilon $$\n",
    "여기서 𝜖ϵ는 𝑁(0,1)를 따르는 정규 분포 난수다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise 대신에 Ornstein-Uhlenbeck noise로 변경한 코드\n",
    "# 주석을 추가한 코드\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################\n",
    "# 1. 데이터 다운로드 및 전처리\n",
    "##############################################\n",
    "# 포트폴리오에 사용할 S&P500 상위 20종목의 티커 리스트 (예시)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\", \n",
    "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
    "\n",
    "# 시작일과 종료일을 설정: 오늘 날짜와 10년 전부터의 데이터를 다운로드\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 각 날짜별로 각 종목의 수익률 계산 (전일 대비 변화율)\n",
    "returns_df = data.pct_change().dropna()\n",
    "\n",
    "##############################################\n",
    "# 2. 벤치마크: 시가총액 비중 배분 계산\n",
    "##############################################\n",
    "# 각 종목의 시가총액 정보를 이용하여 벤치마크 포트폴리오의 비중을 계산\n",
    "market_caps = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        cap = info.get(\"marketCap\", None)\n",
    "        if cap is None:\n",
    "            cap = 1e9\n",
    "        market_caps.append(cap)\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시에도 임의의 값 할당 (데이터 누락 방지)\n",
    "        market_caps.append(1e9)\n",
    "\n",
    "market_caps = np.array(market_caps)\n",
    "# 각 종목의 시가총액을 전체 시가총액 합으로 나누어 벤치마크 비중 계산\n",
    "benchmark_weights = market_caps / market_caps.sum()\n",
    "print(\"Benchmark weights (시가총액 비중):\")\n",
    "for t, w in zip(tickers, benchmark_weights):\n",
    "    print(f\"{t}: {w:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# 3. 환경(Environment) 정의\n",
    "##############################################\n",
    "class HistoricalPortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    과거 일별 수익률 데이터를 순차적으로 제공하는 환경.\n",
    "    - 상태(state): 해당 일자의 각 종목의 수익률 (벡터, shape: [n_assets])\n",
    "    - 행동(action): 각 자산에 할당할 투자 비중. 연속적인 값이며 모든 원소는 0 이상, 합이 1이 되도록 제약됨.\n",
    "      (Actor 네트웤의 softmax 출력 결과를 사용)\n",
    "    - 보상(reward): 해당 일자 포트폴리오의 수익률, 즉 각 종목의 수익률과 투자 비중의 내적(dot product)\n",
    "    \"\"\"\n",
    "    def __init__(self, returns):\n",
    "        super(HistoricalPortfolioEnv, self).__init__()\n",
    "        self.returns = returns  # numpy array 형태로 일별 수익률 데이터, shape=(T, n_assets)\n",
    "        self.n_assets = returns.shape[1]  # 종목의 수\n",
    "        self.current_step = 0  # 에피소드 시작 시점 인덱스 초기화\n",
    "        # 상태 공간: 각 종목의 수익률, 무한대 범위의 실수값 가능\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
    "        # 행동 공간: 각 종목에 할당할 비중, [0,1] 범위이며 실제로 softmax를 통해 정규화됨\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"에피소드 초기화: 시작 단계로 돌아가 현재 상태를 반환\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self.returns[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        주어진 행동(투자 비중)에 따라 보상(당일 수익률)을 계산하고, 다음 상태로 전환.\n",
    "        - 행동이 정규화되지 않은 경우, softmax와 유사하게 정규화해 합이 1이 되도록 보정.\n",
    "        \"\"\"\n",
    "        # 행동 벡터의 합이 0이 되는 것을 방지하기 위해 작은 값 1e-8 추가\n",
    "        weights = action / (np.sum(action) + 1e-8)\n",
    "        # 현재 시점의 종목별 수익률\n",
    "        current_return = self.returns[self.current_step]\n",
    "        # 포트폴리오 수익률은 각 종목 수익률과 투자 비중의 내적\n",
    "        reward = np.dot(weights, current_return)\n",
    "        self.current_step += 1  # 다음 날짜로 이동\n",
    "        # 에피소드 종료 여부 판단: 모든 데이터 소진 시 종료\n",
    "        done = self.current_step >= len(self.returns)\n",
    "        if not done:\n",
    "            next_state = self.returns[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.n_assets)  # 에피소드 종료 시, 임의의 상태 반환\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Gym 환경에서 사용할 수 있도록 pandas DataFrame을 numpy array로 변환\n",
    "returns_np = returns_df.values\n",
    "\n",
    "##############################################\n",
    "# 4. Experience Replay Buffer 구현\n",
    "##############################################\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    ReplayBuffer는 에이전트의 경험(상태, 행동, 보상, 다음 상태, 종료 여부)를 저장하여\n",
    "    미니배치 학습 시 무작위로 샘플링할 수 있도록 지원한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # 버퍼의 최대 저장 크기\n",
    "        self.buffer = []         # 경험 저장 리스트\n",
    "        self.position = 0        # 다음 저장할 인덱스\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        새로운 경험을 버퍼에 저장.\n",
    "        버퍼가 가득 차면 순환 방식으로 기존 경험을 덮어쓴다.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        # 원형 버퍼를 위한 인덱스 업데이트\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def minibatch_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        오리지널 코드는 함수명이 sample인데 기존 random.sample() 메서드랑 동명이라 헷갈려서 내가 바꿈\n",
    "        저장된 경험 중 무작위로 미니배치 샘플링.\n",
    "        반환 값은 각 항목(상태, 행동 등)을 numpy array로 묶어줌.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"현재 버퍼에 저장된 경험의 개수 반환\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################################\n",
    "# 5. DDPG 모델 정의: Actor와 Critic 네트웤\n",
    "##############################################\n",
    "# Actor 네트웤: 상태를 입력받아 각 자산에 투자할 비중(확률 분포)를 출력\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # 출력 계층: 은닉층에서 포트폴리오 비중(로짓 값)으로 변환\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        # softmax를 사용하여 로짓을 확률 분포(합=1, 모든 원소 0 이상)로 변환\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3-softmax\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 출력 계층을 통해 raw logits 생성\n",
    "        logits = self.fc3(x)\n",
    "        # softmax 적용하여 투자 비중으로 정규화\n",
    "        action = self.softmax(logits) # 보장: 모든 원소가 0 이상, 합=1\n",
    "        return action\n",
    "\n",
    "# Critic 네트웤: 주어진 상태와 행동에 대해 Q-value(예상 미래 보상)를 출력\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    # input-fc1-relu-fc2-relu-fc3\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "##############################################\n",
    "# 6. 하이퍼파라미터 및 네트웤 초기화\n",
    "##############################################\n",
    "state_dim = returns_np.shape[1]    # 상태 차원: 자산의 개수\n",
    "action_dim = returns_np.shape[1]     # 행동 차원: 포트폴리오 비중의 개수 (각 자산마다 하나씩)\n",
    "hidden_dim = 64                      # 은닉층 차원\n",
    "\n",
    "# Actor와 Critic 네트웤 인스턴스 생성\n",
    "actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# 타깃 네트웤 생성: 안정적인 학습을 위해 원본 네트웤의 느린 업데이트 버전 사용\n",
    "target_actor = Actor(state_dim, hidden_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "# 초기 타깃 네트웤 파라미터는 원본 네트웤(에서 copy하여)과 동일하게 설정\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "# Optimizer 설정: Actor와 Critic 네트웤 각각에 대해 Adam 옵티마이저 사용\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "# replay_buffer 초기화\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# 학습 관련 기타 하이퍼파라미터 설정\n",
    "num_episodes = 50        # 전체 학습 에피소드 수\n",
    "batch_size = 64          # 미니배치 샘플링 크기\n",
    "gamma = 0.99\n",
    "tau = 0.005              # 타깃 네트웤 soft update 비율 (원본 네트웤와 타깃 네트웤의 파라미터 업데이트 비율)\n",
    "PRINT_INTERVAL = 5\n",
    "\n",
    "# 탐험(exploration)을 위한 노이즈 함수: 행동에 Gaussian 노이즈를 추가한 후 softmax로 정규화\n",
    "# 기존의 add_noise 함수 대신 이름을 add_Gaussian_noise로 변경\n",
    "def add_Gaussian_noise(action, noise_scale=0.1):\n",
    "    noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "    noisy_action = action + noise\n",
    "    # 노이즈가 추가된 행동을 다시 softmax 적용하여 제약 조건(모든 원소 0 이상, 합=1)을 만족\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "# Ornstein-Uhlenbeck noise를 위한 클래스 정의 (내부 상태 유지)\n",
    "# mu, theta, sigma 는 각각 노이즈의 장기 평균, 평균 회귀 속도(rate), 변동성(volatility) 또는 노이즈 강도인데 어떤 값으로 설정하는게 좋을지?\n",
    "# 파라미터 튜닝 필요\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def __call__(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = self.state + dx\n",
    "        return self.state\n",
    "\n",
    "# Ornstein-Uhlenbeck 노이즈를 추가하는 함수\n",
    "def add_ornstein_uhlenbeck_noise(action, ou_noise):\n",
    "    noise = ou_noise()  # OU 노이즈 프로세스를 통해 노이즈 획득\n",
    "    noisy_action = action + noise\n",
    "    exp_action = np.exp(noisy_action)\n",
    "    return exp_action / np.sum(exp_action)\n",
    "\n",
    "##############################################\n",
    "# 7. DDPG 학습 루프\n",
    "##############################################\n",
    "env = HistoricalPortfolioEnv(returns_np)\n",
    "\n",
    "# DDPG 학습 루프 시작 전, OU 노이즈 인스턴스 생성 (행동 차원에 맞춤)\n",
    "ou_noise = OUNoise(action_dim, mu=0.0, theta=0.15, sigma=0.2)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # 에피소드 시작 시 환경 초기화\n",
    "    episode_reward = 0  # 해당 에피소드에서 누적된 보상\n",
    "    done = False\n",
    "    ou_noise.reset()    # 에피소드 시작 시 OU 노이즈 상태 초기화\n",
    "    \n",
    "    while not done:\n",
    "        # 현재 상태를 tensor로 변환하여 Actor 네트웤에 입력\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        actor.eval()  # 평가 모드로 전환 (dropout, batchnorm 등이 있을 경우 영향을 줄 수 있음)\n",
    "        with torch.no_grad():\n",
    "            # Actor 네트웤가 출력한 투자 비중 (이미 softmax 적용됨)\n",
    "            action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        actor.train()  # 다시 학습 모드로 전환\n",
    "        \n",
    "        \"\"\"\n",
    "        # 학습 시, 탐험을 위해 Gaussian 노이즈 추가\n",
    "        action = add_Gaussian_noise(action, noise_scale=0.1)\n",
    "        \"\"\"\n",
    "        # 기존 Gaussian 노이즈 대신 Ornstein-Uhlenbeck 노이즈 적용\n",
    "        action = add_ornstein_uhlenbeck_noise(action, ou_noise)\n",
    "        \n",
    "        # 환경에서 다음 상태, 보상, 종료 여부 획득\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # replay buffer 에 현재 경험 저장\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # 충분한 경험이 쌓였으면, 미니배치로 네트웤 업데이트 수행\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # 미니배치 샘플링\n",
    "            states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.minibatch_sample(batch_size)\n",
    "            \n",
    "            # numpy array를 torch tensor로 변환\n",
    "            states_b = torch.FloatTensor(states_b)\n",
    "            actions_b = torch.FloatTensor(actions_b)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1)\n",
    "            next_states_b = torch.FloatTensor(next_states_b)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1)\n",
    "            \n",
    "            # Critic 업데이트: 목표 Q-value 계산 및 MSE 손실 최소화\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states_b) # 타깃 Actor를 사용해 다음 상태에서의 행동 예측\n",
    "                target_q = target_critic(next_states_b, next_actions) # 타깃 Critic으로 다음 상태에서의 Q-value 계산\n",
    "                # Bellman equation에 따른 목표 Q-value: 현재 보상 + 할인율 * 미래 Q-value * (종료 여부 반영)\n",
    "                y = rewards_b + gamma * target_q * (1 - dones_b)\n",
    "            \n",
    "            # 현재 Critic 네트웤의 Q-value와 MSE-loss 계산\n",
    "            current_q = critic(states_b, actions_b)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            \n",
    "            # Critic 네트웤 파라미터 업데이트\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor 업데이트: 상태에서의 행동이 Critic 네트웤에서 높은 Q-value를 유도하도록 업데이트\n",
    "            actor_loss = -critic(states_b, actor(states_b)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # 타깃 네트웤 soft update: 원본 네트웤 파라미터의 일부를 타깃 네트웤에 반영하여 천천히 업데이트\n",
    "            \"\"\" 오리지널 코드인데 순서를 아래처럼 깔끔하게 변경했다\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \"\"\"\n",
    "            \n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    # 에피소드 진행 중 PRINT_INTERVAL 에피소드마다 결과 출력\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f\"Episode {episode:3d} | Episode Reward: {episode_reward:.4f}\")\n",
    "\n",
    "print(\"DDPG 학습 완료\")\n",
    "\n",
    "##############################################\n",
    "# 8. 백테스팅: 학습된 Actor 네트웤으로 전체 기간에 대해 포트폴리오 수익률 산출\n",
    "##############################################\n",
    "def backtest(actor_model, returns_np):\n",
    "    \"\"\"\n",
    "    주어진 학습된 Actor 네트웤을 사용하여 전체 백테스트 기간 동안의 일별 포트폴리오 수익률을 산출.\n",
    "    - 결정론적 행동 사용: 탐험 노이즈 없이 Actor의 출력(softmax 적용된 투자 비중)을 그대로 사용.\n",
    "    \"\"\"\n",
    "    actor_model.eval()  # 평가 모드로 전환\n",
    "    env_bt = HistoricalPortfolioEnv(returns_np)  # 백테스트용 환경 초기화\n",
    "    state = env_bt.reset()\n",
    "    daily_returns = []\n",
    "    while True:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            # 결정론적 행동 산출 (탐험 없이)\n",
    "            action = actor_model(state_tensor).cpu().data.numpy().flatten()\n",
    "        \n",
    "        next_state, reward, done, _ = env_bt.step(action) # 환경에서 다음 상태, 보상, 종료 여부 획득\n",
    "        daily_returns.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return np.array(daily_returns)\n",
    "\n",
    "# 백테스트: 학습된 DDPG Actor를 사용한 일별 수익률 산출\n",
    "ddpg_daily_returns = backtest(actor, returns_np)\n",
    "# 누적 수익률 계산: 매일의 수익률을 누적 곱하여 포트폴리오 성장 곡선 산출\n",
    "ddpg_cumulative = np.cumprod(1 + ddpg_daily_returns)\n",
    "\n",
    "##############################################\n",
    "# 9. 벤치마크 백테스팅: BM (시가총액 비중 포트폴리오)\n",
    "##############################################\n",
    "# 벤치마크 포트폴리오의 일별 수익률 계산:\n",
    "# 각 종목의 수익률에 시가총액 비중을 곱한 후 합산\n",
    "benchmark_daily = returns_df.dot(benchmark_weights).values[-len(ddpg_daily_returns):]\n",
    "# 누적 수익률 계산\n",
    "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
    "\n",
    "# 백테스트 기간 날짜 (returns_df의 index 중 마지막 부분을 사용)\n",
    "dates = returns_df.index[-len(ddpg_daily_returns):]\n",
    "\n",
    "##############################################\n",
    "# 10. 결과 Plot: BM vs DDPG\n",
    "##############################################\n",
    "plt.figure(figsize=(12,6))\n",
    "# 벤치마크 포트폴리오의 누적 수익률 그래프 (점선 스타일)\n",
    "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (BM)\", linestyle=\"--\")\n",
    "# DDPG 에이전트 포트폴리오의 누적 수익률 그래프\n",
    "plt.plot(dates, ddpg_cumulative, label=\"DDPG Agent\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Cumulative returns\")\n",
    "plt.title(\"Backtesting: BM vs DDPG\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
