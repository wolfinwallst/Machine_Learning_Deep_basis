{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfinwallst/Machine_Learning_Deep_basis/blob/main/RL_A2C_portfolio_optimization_PoC_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PsCA-SXw6sNc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIQfIVl16sNd",
        "outputId": "5d76bbd8-7ad4-46d1-f2e4-6181c4bbcadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  8 of 8 completed\n",
            "ERROR:yfinance:\n",
            "8 Failed downloads:\n",
            "ERROR:yfinance:['AAPL', 'BRK-B', 'META', 'MSFT', 'NVDA', 'GOOGL', 'AMZN', 'TSLA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        }
      ],
      "source": [
        "##############################################\n",
        "# 1. 데이터 다운로드 및 전처리\n",
        "##############################################\n",
        "# S&P500 상위 20종목 (예시 티커)\n",
        "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\",\n",
        "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
        "\n",
        "# 10년 전부터 오늘까지의 데이터\n",
        "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')\n",
        "\n",
        "data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# 일별 수익률 계산\n",
        "returns_df = data.pct_change().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTv5XRkS6sNe"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 2. 벤치마크: 시가총액 비중 배분 계산\n",
        "##############################################\n",
        "market_caps = []\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        cap = info.get(\"marketCap\", None)\n",
        "        # marketCap 정보가 없으면 임의의 값 할당\n",
        "        if cap is None:\n",
        "            cap = 1e9\n",
        "        market_caps.append(cap)\n",
        "    except Exception as e:\n",
        "        market_caps.append(1e9)\n",
        "\n",
        "market_caps = np.array(market_caps)\n",
        "benchmark_weights = market_caps / market_caps.sum()\n",
        "print(\"Benchmark weights (시가총액 비중):\")\n",
        "print(dict(zip(tickers, np.round(benchmark_weights, 4))))\n",
        "\n",
        "# 벤치마크의 일별 수익률 = 각 종목의 일별 수익률에 시가총액 비중 곱한 값의 합산\n",
        "benchmark_daily_returns = returns_df.dot(benchmark_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2aFnIpq6sNe"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 3. 환경(Environment) 정의: HistoricalPortfolioEnv\n",
        "##############################################\n",
        "# Gym의 Env 클래스를 상속받아, 과거 수익률 데이터를 순차적으로 제공하는 포트폴리오 환경을 정의\n",
        "class HistoricalPortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    과거 일별 수익률 데이터를 순차적으로 제공하는 환경.\n",
        "    - 상태(state): 해당일의 각 종목 수익률 (vector)\n",
        "    - 행동(action): 각 종목에 할당할 투자비중 (합=1)\n",
        "    - 보상(reward): 선택한 투자비중과 실제 수익률의 내적 (포트폴리오 당일 수익률)\n",
        "    \"\"\"\n",
        "    def __init__(self, returns):\n",
        "        super(HistoricalPortfolioEnv, self).__init__()\n",
        "        self.returns = returns  # numpy array, shape=(T, n_assets)\n",
        "        self.n_assets = returns.shape[1]\n",
        "        self.current_step = 0\n",
        "        # 행동 공간: 각 종목에 대한 투자 비중, 0~1 범위의 연속값 (벡터)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
        "        # 관측 공간: 각 종목의 수익률 (실수 값으로 무한대 범위)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        # 환경 초기화: 시간 스텝을 0으로 초기화 후 첫 번째 수익률 벡터 반환\n",
        "        self.current_step = 0\n",
        "        return self.returns[self.current_step]\n",
        "\n",
        "    def step(self, action):\n",
        "        # 행동값 정규화 (합=1) & 1e-8은 분모가 0 이 되는 상황을 방지코자\n",
        "        weights = action / (np.sum(action) + 1e-8)\n",
        "        current_return = self.returns[self.current_step] # 현재 스텝의 종목별 수익률\n",
        "        reward = np.dot(weights, current_return) # 포트폴리오 수익률\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.returns) # 마지막 스텝인지 확인\n",
        "\n",
        "        if not done:\n",
        "            next_state = self.returns[self.current_step]\n",
        "        else:\n",
        "            next_state = np.zeros(self.n_assets)\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "# 수익률 데이터를 numpy 배열로 변환 후 환경 생성\n",
        "returns_np = returns_df.values\n",
        "env = HistoricalPortfolioEnv(returns_np)\n",
        "n_assets = env.n_assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6nr-54W6sNe"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 4. Actor-Critic 모델 정의 (PyTorch)\n",
        "##############################################\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # Actor: Dirichlet 분포의 알파 파라미터 출력 (양수여야 하므로 softplus 후 +1)\n",
        "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
        "        # Critic 출력층: 상태의 가치를 예측하는 선형층 (스칼라 출력)\n",
        "        self.critic = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # actor 네트웤 순서: input-fc1-relu-fc2-relu-actor\n",
        "        actor_logits = self.actor(x)    # policy network 출력\n",
        "        # critic 네트웤 순서: input-fc1-relu-fc2-relu-critic\n",
        "        value = self.critic(x)          # state function 출력\n",
        "        return actor_logits, value\n",
        "\n",
        "input_dim = n_assets    # 입력 차원: 각 종목의 수익률 (자산 개수)\n",
        "hidden_dim = 64         # 은닉층 차원\n",
        "output_dim = n_assets   # 출력 차원: 각 종목에 대한 투자 비중 예측\n",
        "\n",
        "model = ActorCritic(input_dim, hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "gamma = 0.99 # discount factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf0XIVpS6sNf"
      },
      "source": [
        "궁금증\n",
        "1) actor 네트웤 순서: `input-fc1-relu-fc2-relu-actor` 로 잡았는데\n",
        "`RL_Actor_Critic_CartPole_01.ipynb` 처럼 `input-fc1-relu-fc2-softmax` 로 바꾼다면?\n",
        "\n",
        "2) hidden_dim 을 얼마나 크게 / 작게 잡는게 좋은가?\n",
        "3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzAZnMk36sNf"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 5. 학습 (Training) - Actor-Critic 방식\n",
        "##############################################\n",
        "num_episodes = 50  # 에피소드 수 (데모 목적)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()    # 환경 초기화 및 첫 상태 획득\n",
        "    log_probs = []         # 각 스텝의 log-확률 저장 리스트 (Actor 업데이트용)\n",
        "    values = []            # Critic의 상태 가치 예측 저장 리스트\n",
        "    rewards = []           # 에피소드 동안의 보상 저장 리스트\n",
        "    done = False           # 에피소드 종료 여부 플래그\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # [1, n_assets]\n",
        "        logits, value = model(state_tensor)\n",
        "        # Dirichlet 분포의 알파 파라미터 계산\n",
        "        alpha = F.softplus(logits) + 1.0\n",
        "        dist = torch.distributions.Dirichlet(alpha.view(-1))\n",
        "        # 디리클레 분포에서 샘플링하여 행동(투자 비중) 결정\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
        "\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value.squeeze(0))\n",
        "        rewards.append(reward)\n",
        "        state = next_state\n",
        "\n",
        "    # 에피소드별 누적수익률 (Return) 계산 (감가율 적용)\n",
        "    returns_episode = []\n",
        "    R = 0\n",
        "    for r in rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns_episode.insert(0, R)\n",
        "    returns_episode = torch.tensor(returns_episode, dtype=torch.float32)\n",
        "    values = torch.stack(values)\n",
        "    log_probs = torch.stack(log_probs)\n",
        "\n",
        "    advantages = returns_episode - values.detach() # Advantage 계산: 실제 누적 보상과 Critic의 예측 가치 차이\n",
        "    actor_loss = -(log_probs * advantages).mean() # Actor 손실: 로그 확률에 Advantage를 곱해 부호 반전 후 평균 (정책 경사법)\n",
        "    critic_loss = F.mse_loss(values, returns_episode) # Critic 손실: 가치 예측과 실제 누적 보상의 평균 제곱 오차(MSE)\n",
        "    loss = actor_loss + critic_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        total_reward = sum(rewards)\n",
        "        print(f\"Episode {episode:3d} | Loss: {loss.item():.4f} | Total Reward: {total_reward:.4f}\")\n",
        "\n",
        "print(\"학습 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLF7gTf6sNg"
      },
      "source": [
        "1) 랜덤 action 을 결정할 때, Dirichlet dist.를 사용했는데 다른 방법은? 그리고 현재 상태에서 디리클레 분포를 사용해서 랜덤 action을 결정하는 방법이 얼마나 적합한가?\n",
        "\n",
        "ChatGPT: 1. Dirichlet 분포 외의 대안\n",
        "Softmax 기반 접근법 (Gumbel-Softmax 포함)\n",
        "\n",
        "방법: 네트워크가 출력한 로짓(logit) 값에 대해 softmax 연산을 적용하면 0~1 사이의 값들이 나오고, 총합이 1이 됩니다.\n",
        "장점: 구현이 간단하며, 확률 분포처럼 해석할 수 있습니다.\n",
        "단점: 기본 softmax는 결정론적이어서 탐험성이 부족할 수 있으므로, Gumbel-Softmax와 같이 샘플링 기법을 도입하여 확률적 행동 선택을 유도할 수 있습니다.\n",
        "Logit-Normal 분포\n",
        "\n",
        "방법: 네트워크가 출력한 값을 가우시안 분포의 logit으로 해석하고, softmax를 통해 단순화된 분포로 변환하는 방법입니다.\n",
        "장점: 연속적 확률 공간을 모델링할 수 있으며, Dirichlet와는 다른 분산 특성을 가질 수 있습니다.\n",
        "단점: 파라미터 튜닝과 학습 안정성 측면에서 Dirichlet에 비해 다소 복잡할 수 있습니다.\n",
        "Stick-Breaking Process\n",
        "\n",
        "방법: 단순한 재귀적 방법으로 “막대기를 나누는” 과정(Stick-breaking)을 통해 단순체(Simplex) 상의 점을 생성합니다.\n",
        "장점: 자연스럽게 단순체 제약을 만족시키며, 다양한 분포 형태를 모델링할 수 있습니다.\n",
        "단점: 구현 복잡성이 증가하고, 미분가능성 문제 때문에 직접적인 학습에 사용하기 어렵습니다.\n",
        "2. Dirichlet 분포 사용 적합성 평가\n",
        "Dirichlet 분포가 현재 포트폴리오 배분 문제에서 랜덤 액션 결정에 적합한지를 평가할 때 고려할 기준은 다음과 같습니다.\n",
        "\n",
        "제약 조건 만족:\n",
        "\n",
        "판단 기준: 포트폴리오의 각 비중이 0 이상이며, 합이 1이어야 한다는 조건\n",
        "판단 정도: Dirichlet 분포는 이 제약을 내재적으로 만족시키므로 매우 강한 적합성을 보입니다.\n",
        "분포의 유연성 및 탐험 성질 (Entropy):\n",
        "\n",
        "판단 기준: 네트워크가 출력하는 alpha 파라미터에 따라 분포의 집중도(Entropy)가 결정됩니다.\n",
        "낮은 alpha: 분포가 넓게 퍼져(높은 entropy) 탐험성이 큼\n",
        "높은 alpha: 분포가 집중되어(낮은 entropy) 결정론적으로 행동 선택\n",
        "판단 정도: 적절한 alpha 값을 조절한다면, Dirichlet 분포는 탐험과 활용 사이의 균형을 잘 맞출 수 있습니다. (탐험성이 충분하면서도 제약 조건을 유지하는 점에서 중간 이상으로 적합)\n",
        "학습 안정성 및 미분 가능성:\n",
        "\n",
        "판단 기준: 정책 네트워크가 반환하는 파라미터로부터 샘플링한 행동에 대해 역전파가 원활하게 진행되어야 합니다.\n",
        "판단 정도: Dirichlet 분포는 미분 가능하게 파라미터화할 수 있으므로, 학습 측면에서는 안정적이라고 볼 수 있습니다.\n",
        "실제 성능 (Empirical Performance):\n",
        "\n",
        "판단 기준: 백테스팅이나 시뮬레이션 결과를 통해, 실제 투자 성과나 누적 수익률의 안정성이 평가되어야 합니다.\n",
        "판단 정도: 초기 실험 결과가 긍정적이라면, Dirichlet 분포 사용은 문제에 잘 맞는다고 판단할 수 있습니다. 다만, 하이퍼파라미터(특히 alpha 값의 초기값 및 업데이트 방식) 튜닝이 중요합니다.\n",
        "결론\n",
        "대안 방법: Softmax (또는 Gumbel-Softmax), Logit-Normal 분포, Stick-Breaking Process 등이 있으며, 각각의 방법은 구현 복잡성, 학습 안정성, 그리고 탐험 성향에서 차이가 있습니다.\n",
        "Dirichlet 분포 평가:\n",
        "적합성: 매우 높음 (특히 제약 조건 내재 만족 측면)\n",
        "탐험/활용 균형: 네트워크의 alpha 파라미터 조절에 따라 결정되며, 적절하게 설정하면 중간 이상으로 적합함\n",
        "판단 강도: 현재 포트폴리오 배분 문제에서 Dirichlet 분포는 강력히 추천할 수 있는 방법으로 판단되며, 실제 성능을 면밀히 관찰하면서 하이퍼파라미터 튜닝을 진행하는 것이 좋습니다.\n",
        "따라서, Dirichlet 분포는 포트폴리오 배분의 제약 조건을 자연스럽게 만족시키고, 충분한 탐험성을 보장할 수 있기 때문에 적합한 선택이라 할 수 있습니다. 다만, 다른 방법들도 상황에 따라 유용할 수 있으므로, 실제 프로젝트에서는 다양한 접근법을 비교 평가하는 것이 바람직합니다.\n",
        "\n",
        "2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN9a6Urr6sNg"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 6. 백테스팅 (Backtesting)\n",
        "##############################################\n",
        "# 학습된 모델을 사용하여 전체 기간에 대해 포트폴리오 수익률 산출\n",
        "\n",
        "# 백테스트를 위한 새로운 환경 인스턴스 생성\n",
        "env_bt = HistoricalPortfolioEnv(returns_np)\n",
        "state = env_bt.reset()\n",
        "agent_daily_returns = []  # 에이전트가 생성한 일별 수익률 기록 리스트\n",
        "\n",
        "while True:\n",
        "    # 현재 상태를 텐서로 변환하여 모델 입력으로 사용\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        # 모델을 통해 정책 네트워크의 출력 계산 (액션 결정)\n",
        "        logits, _ = model(state_tensor)\n",
        "        # Dirichlet 분포 파라미터 계산 (softplus 후 +1)\n",
        "        alpha = F.softplus(logits) + 1.0\n",
        "        dist = torch.distributions.Dirichlet(alpha.view(-1))\n",
        "        # 백테스트에서는 확률 분포의 평균값을 사용하여 결정론적 정책으로 행동 결정\n",
        "        weights = dist.mean\n",
        "    # 선택한 투자 비중을 환경에 적용하여 다음 상태와 보상 획득\n",
        "    next_state, reward, done, _ = env_bt.step(weights.numpy())\n",
        "    agent_daily_returns.append(reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "# 에이전트의 일별 수익률을 numpy 배열로 변환 후 누적 수익률 계산 (초기 투자금 1 기준)\n",
        "agent_daily_returns = np.array(agent_daily_returns)\n",
        "agent_cumulative = np.cumprod(1 + agent_daily_returns)\n",
        "\n",
        "# 벤치마크 누적 수익률 계산: 벤치마크 일별 수익률을 같은 기간에 맞추어 계산\n",
        "benchmark_daily = benchmark_daily_returns.values[-len(agent_daily_returns):]\n",
        "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
        "\n",
        "# 백테스트 기간의 날짜 (원본 수익률 데이터의 인덱스 사용)\n",
        "dates = returns_df.index[-len(agent_daily_returns):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BU9pRDT6sNg"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 7. 결과 Plot: RL 에이전트 vs. Benchmark\n",
        "##############################################\n",
        "# 백테스팅 결과를 시각화하여 RL 에이전트와 벤치마크(시가총액 비중)의 누적 수익률 비교\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(dates, agent_cumulative, label=\"RL Agent Portfolio\")\n",
        "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (시가총액 비중)\", linestyle=\"--\")\n",
        "plt.xlabel(\"날짜\")\n",
        "plt.ylabel(\"누적 수익률\")\n",
        "plt.title(\"Backtesting: RL Agent vs. Benchmark\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DtxTU-E6sNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJWWBNvH6sNg"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf          # 금융 데이터 다운로드를 위한 라이브러리\n",
        "import numpy as np             # 수치 연산을 위한 라이브러리\n",
        "import pandas as pd            # 데이터프레임 처리 라이브러리\n",
        "import matplotlib.pyplot as plt# 시각화를 위한 라이브러리\n",
        "import datetime                # 날짜 및 시간 계산을 위한 모듈\n",
        "import gym                     # 강화학습 환경을 구축하기 위한 라이브러리\n",
        "from gym import spaces         # Gym 환경의 행동 및 관측 공간 정의에 사용\n",
        "import torch                   # 파이토치(PyTorch): 딥러닝 모델 구현용\n",
        "import torch.nn as nn          # 신경망 계층 모듈\n",
        "import torch.nn.functional as F# 활성화 함수 등 함수형 API 제공\n",
        "import torch.optim as optim    # 최적화 알고리즘 모듈\n",
        "\n",
        "##############################################\n",
        "# 1. 데이터 다운로드 및 전처리\n",
        "##############################################\n",
        "# S&P500 상위 20종목 티커 리스트 (예시)\n",
        "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\", \"BRK-B\",\n",
        "           \"JNJ\", \"UNH\", \"V\", \"MA\", \"HD\", \"PG\", \"JPM\", \"BAC\", \"VZ\", \"DIS\", \"PFE\", \"XOM\"]\n",
        "\n",
        "# 현재 날짜와 10년 전 날짜를 계산하여 데이터 기간 설정\n",
        "end_date = datetime.datetime.now().strftime('%Y-%m-%d')  # 현재 날짜 (종료일)\n",
        "start_date = (datetime.datetime.now() - datetime.timedelta(days=365*10)).strftime('%Y-%m-%d')  # 10년 전 날짜 (시작일)\n",
        "\n",
        "# yfinance를 사용하여 상위 20종목의 Adjusted Close 가격 데이터를 다운로드\n",
        "data = yf.download(tickers, start=start_date, end=end_date)[\"Adj Close\"]\n",
        "data.dropna(inplace=True)  # 결측치가 포함된 행 제거\n",
        "\n",
        "# 일별 수익률 계산: 각 날짜별 종가 변화율을 계산하여 수익률 데이터 생성\n",
        "returns_df = data.pct_change().dropna()\n",
        "\n",
        "##############################################\n",
        "# 2. 벤치마크: 시가총액 비중 배분 계산\n",
        "##############################################\n",
        "# 각 종목의 시가총액 정보를 가져와 벤치마크 포트폴리오의 가중치(비중)를 계산\n",
        "market_caps = []\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info  # 해당 종목의 정보 조회\n",
        "        cap = info.get(\"marketCap\", None)  # 시가총액 정보 추출\n",
        "        # 만약 시가총액 정보가 없으면 기본값(예: 10억)을 할당\n",
        "        if cap is None:\n",
        "            cap = 1e9\n",
        "        market_caps.append(cap)\n",
        "    except Exception as e:\n",
        "        # 정보 조회 중 오류가 발생하면 기본값 할당\n",
        "        market_caps.append(1e9)\n",
        "\n",
        "# 시가총액 배열을 numpy 배열로 변환하고, 전체 시가총액에 대한 각 종목의 비중 계산\n",
        "market_caps = np.array(market_caps)\n",
        "benchmark_weights = market_caps / market_caps.sum()\n",
        "\n",
        "print(\"Benchmark weights (시가총액 비중):\")\n",
        "print(dict(zip(tickers, np.round(benchmark_weights, 4))))\n",
        "\n",
        "# 각 종목의 일별 수익률에 시가총액 비중을 곱하여 벤치마크의 일별 수익률 계산\n",
        "benchmark_daily_returns = returns_df.dot(benchmark_weights)\n",
        "\n",
        "##############################################\n",
        "# 3. 환경(Environment) 정의: HistoricalPortfolioEnv\n",
        "##############################################\n",
        "# Gym의 Env 클래스를 상속받아, 과거 수익률 데이터를 순차적으로 제공하는 포트폴리오 환경을 정의\n",
        "class HistoricalPortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    과거 일별 수익률 데이터를 순차적으로 제공하는 환경.\n",
        "    - 상태(state): 해당일의 각 종목의 수익률 (벡터 형태)\n",
        "    - 행동(action): 각 종목에 할당할 투자 비중 (총합=1이어야 함)\n",
        "    - 보상(reward): 선택한 투자 비중과 실제 수익률의 내적 (해당일 포트폴리오 수익률)\n",
        "    \"\"\"\n",
        "    def __init__(self, returns):\n",
        "        super(HistoricalPortfolioEnv, self).__init__()\n",
        "        self.returns = returns            # numpy 배열 형태의 수익률 데이터, shape=(T, n_assets)\n",
        "        self.n_assets = returns.shape[1]    # 자산(종목) 개수\n",
        "        self.current_step = 0               # 데이터의 현재 시간 스텝\n",
        "        # 행동 공간: 각 종목에 대한 투자 비중, 0~1 범위의 연속값 (벡터)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
        "        # 관측 공간: 각 종목의 수익률 (실수 값으로 무한대 범위)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        # 환경 초기화: 시간 스텝을 0으로 초기화 후 첫 번째 수익률 벡터 반환\n",
        "        self.current_step = 0\n",
        "        return self.returns[self.current_step]\n",
        "\n",
        "    def step(self, action):\n",
        "        # 행동값을 정규화하여 총합이 1이 되도록 함\n",
        "        weights = action / (np.sum(action) + 1e-8)\n",
        "        # 현재 스텝의 종목별 수익률 가져오기\n",
        "        current_return = self.returns[self.current_step]\n",
        "        # 포트폴리오 수익률: 선택한 투자 비중과 실제 수익률의 내적 계산\n",
        "        reward = np.dot(weights, current_return)\n",
        "        self.current_step += 1  # 다음 시간 스텝으로 이동\n",
        "        done = self.current_step >= len(self.returns)  # 마지막 스텝인지 확인\n",
        "        if not done:\n",
        "            next_state = self.returns[self.current_step]\n",
        "        else:\n",
        "            next_state = np.zeros(self.n_assets)  # 에피소드 종료 시 더 이상 관측 불가\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "# 수익률 데이터를 numpy 배열로 변환 후 환경 생성\n",
        "returns_np = returns_df.values\n",
        "env = HistoricalPortfolioEnv(returns_np)\n",
        "n_assets = env.n_assets  # 환경 내 자산(종목) 수\n",
        "\n",
        "##############################################\n",
        "# 4. Actor-Critic 모델 정의 (PyTorch)\n",
        "##############################################\n",
        "# Actor-Critic 네트워크 정의: 정책(Actor)와 가치(Critic)를 동시에 학습\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # 입력층에서 은닉층1로 연결하는 선형층\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # 은닉층1에서 은닉층2로 연결하는 선형층\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # Actor 출력층: Dirichlet 분포의 알파 파라미터를 예측 (양의 값이어야 하므로 softplus 후 +1)\n",
        "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
        "        # Critic 출력층: 상태의 가치를 예측하는 선형층 (스칼라 출력)\n",
        "        self.critic = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 데이터를 은닉층을 통과시키며 ReLU 활성화 함수 적용\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # Actor와 Critic의 결과값 계산\n",
        "        actor_logits = self.actor(x)  # 정책 네트워크의 출력\n",
        "        value = self.critic(x)          # 가치 함수의 출력\n",
        "        return actor_logits, value\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "input_dim = n_assets    # 입력 차원: 각 종목의 수익률 (자산 개수)\n",
        "hidden_dim = 64         # 은닉층 차원\n",
        "output_dim = n_assets   # 출력 차원: 각 종목에 대한 투자 비중 예측\n",
        "\n",
        "# Actor-Critic 모델 인스턴스 생성 및 Adam 옵티마이저 설정\n",
        "model = ActorCritic(input_dim, hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "gamma = 0.99  # 미래 보상 감가율 (discount factor)\n",
        "\n",
        "##############################################\n",
        "# 5. 학습 (Training) - Actor-Critic 방식\n",
        "##############################################\n",
        "num_episodes = 50  # 총 에피소드 수 (데모 목적)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()    # 환경 초기화 및 첫 상태 획득\n",
        "    log_probs = []         # 각 스텝의 로그 확률 저장 리스트 (Actor 업데이트용)\n",
        "    values = []            # Critic의 상태 가치 예측 저장 리스트\n",
        "    rewards = []           # 에피소드 동안의 보상 저장 리스트\n",
        "    done = False           # 에피소드 종료 여부 플래그\n",
        "\n",
        "    while not done:\n",
        "        # 현재 상태를 파이토치 텐서로 변환 (배치 차원 추가)\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # shape: [1, n_assets]\n",
        "        # 모델을 통해 정책(Actor)와 가치(Critic)를 계산\n",
        "        logits, value = model(state_tensor)\n",
        "        # Dirichlet 분포의 알파 파라미터 계산: softplus 함수를 사용해 음수가 없도록 보정 후 +1\n",
        "        alpha = F.softplus(logits) + 1.0\n",
        "        # Dirichlet 분포 생성 (1차원 벡터 형태)\n",
        "        dist = torch.distributions.Dirichlet(alpha.view(-1))\n",
        "        # 분포에서 샘플링하여 행동(투자 비중) 결정\n",
        "        action = dist.sample()\n",
        "        # 선택된 행동의 로그 확률 계산 (학습 시 정책 기울기 업데이트에 사용)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        # 환경에 행동 적용 후 다음 상태, 보상, 종료 여부 획득\n",
        "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
        "\n",
        "        # 각 변수들 기록\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value.squeeze(0))\n",
        "        rewards.append(reward)\n",
        "        state = next_state  # 상태 업데이트\n",
        "\n",
        "    # 에피소드의 누적 보상(리턴)을 감가율(gamma)을 고려해 계산 (역순 계산)\n",
        "    returns_episode = []\n",
        "    R = 0\n",
        "    for r in rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns_episode.insert(0, R)\n",
        "    returns_episode = torch.tensor(returns_episode, dtype=torch.float32)\n",
        "    values = torch.stack(values)\n",
        "    log_probs = torch.stack(log_probs)\n",
        "\n",
        "    # Advantage 계산: 실제 누적 보상과 Critic의 예측 가치 차이\n",
        "    advantages = returns_episode - values.detach()\n",
        "    # Actor 손실: 로그 확률에 Advantage를 곱해 부호 반전 후 평균 (정책 경사법)\n",
        "    actor_loss = -(log_probs * advantages).mean()\n",
        "    # Critic 손실: 가치 예측과 실제 누적 보상의 평균 제곱 오차(MSE)\n",
        "    critic_loss = F.mse_loss(values, returns_episode)\n",
        "    # 최종 손실: Actor 손실과 Critic 손실의 합\n",
        "    loss = actor_loss + critic_loss\n",
        "\n",
        "    # 역전파 및 파라미터 업데이트\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 10 에피소드마다 학습 상태(손실, 총 보상) 출력\n",
        "    if episode % 10 == 0:\n",
        "        total_reward = sum(rewards)\n",
        "        print(f\"Episode {episode:3d} | Loss: {loss.item():.4f} | Total Reward: {total_reward:.4f}\")\n",
        "\n",
        "print(\"학습 완료\")\n",
        "\n",
        "##############################################\n",
        "# 6. 백테스팅 (Backtesting)\n",
        "##############################################\n",
        "# 학습된 모델을 사용하여 전체 기간에 대해 포트폴리오 수익률 산출\n",
        "\n",
        "# 백테스트를 위한 새로운 환경 인스턴스 생성\n",
        "env_bt = HistoricalPortfolioEnv(returns_np)\n",
        "state = env_bt.reset()\n",
        "agent_daily_returns = []  # 에이전트가 생성한 일별 수익률 기록 리스트\n",
        "\n",
        "while True:\n",
        "    # 현재 상태를 텐서로 변환하여 모델 입력으로 사용\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        # 모델을 통해 정책 네트워크의 출력 계산 (액션 결정)\n",
        "        logits, _ = model(state_tensor)\n",
        "        # Dirichlet 분포 파라미터 계산 (softplus 후 +1)\n",
        "        alpha = F.softplus(logits) + 1.0\n",
        "        dist = torch.distributions.Dirichlet(alpha.view(-1))\n",
        "        # 백테스트에서는 확률 분포의 평균값을 사용하여 결정론적 정책으로 행동 결정\n",
        "        weights = dist.mean\n",
        "    # 선택한 투자 비중을 환경에 적용하여 다음 상태와 보상 획득\n",
        "    next_state, reward, done, _ = env_bt.step(weights.numpy())\n",
        "    agent_daily_returns.append(reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "# 에이전트의 일별 수익률을 numpy 배열로 변환 후 누적 수익률 계산 (초기 투자금 1 기준)\n",
        "agent_daily_returns = np.array(agent_daily_returns)\n",
        "agent_cumulative = np.cumprod(1 + agent_daily_returns)\n",
        "\n",
        "# 벤치마크 누적 수익률 계산: 벤치마크 일별 수익률을 같은 기간에 맞추어 계산\n",
        "benchmark_daily = benchmark_daily_returns.values[-len(agent_daily_returns):]\n",
        "benchmark_cumulative = np.cumprod(1 + benchmark_daily)\n",
        "\n",
        "# 백테스트 기간의 날짜 (원본 수익률 데이터의 인덱스 사용)\n",
        "dates = returns_df.index[-len(agent_daily_returns):]\n",
        "\n",
        "##############################################\n",
        "# 7. 결과 Plot: RL 에이전트 vs. Benchmark\n",
        "##############################################\n",
        "# 백테스팅 결과를 시각화하여 RL 에이전트와 벤치마크(시가총액 비중)의 누적 수익률 비교\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(dates, agent_cumulative, label=\"RL Agent Portfolio\")\n",
        "plt.plot(dates, benchmark_cumulative, label=\"Benchmark (시가총액 비중)\", linestyle=\"--\")\n",
        "plt.xlabel(\"날짜\")\n",
        "plt.ylabel(\"누적 수익률\")\n",
        "plt.title(\"Backtesting: RL Agent vs. Benchmark\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}